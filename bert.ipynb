{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: psutil in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from accelerate) (0.19.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from accelerate) (0.4.1)\n",
      "Requirement already satisfied: filelock in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from huggingface-hub->accelerate) (2023.12.1)\n",
      "Requirement already satisfied: requests in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Applications/anaconda3/envs/3350 /lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and Setup\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import fightinwords as fw\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from transformers import DistilBertTokenizer, DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Right and Left Labels Creation From Congress Bills & Presidential Debates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Presidential Debates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>year</th>\n",
       "      <th>dem/rep</th>\n",
       "      <th>left/right</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PENCE</td>\n",
       "      <td>2020</td>\n",
       "      <td>rep</td>\n",
       "      <td>right</td>\n",
       "      <td>Well thank you for the question, but I’ll use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PENCE</td>\n",
       "      <td>2020</td>\n",
       "      <td>rep</td>\n",
       "      <td>right</td>\n",
       "      <td>My hope is that when the hearing takes place,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PENCE</td>\n",
       "      <td>2020</td>\n",
       "      <td>rep</td>\n",
       "      <td>right</td>\n",
       "      <td>– treated respectfully and voted and confirme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HARRIS</td>\n",
       "      <td>2020</td>\n",
       "      <td>dem</td>\n",
       "      <td>left</td>\n",
       "      <td>Thank you, Susan. First of all, Joe Biden and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PENCE</td>\n",
       "      <td>2020</td>\n",
       "      <td>rep</td>\n",
       "      <td>right</td>\n",
       "      <td>Well, thank you, Susan. Let me just say, addr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>BUSH</td>\n",
       "      <td>2004</td>\n",
       "      <td>rep</td>\n",
       "      <td>right</td>\n",
       "      <td>Take, for example, the ban on partial birth ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>BUSH</td>\n",
       "      <td>2004</td>\n",
       "      <td>rep</td>\n",
       "      <td>right</td>\n",
       "      <td>What I’m saying is, is that as we promote life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>BUSH</td>\n",
       "      <td>2004</td>\n",
       "      <td>rep</td>\n",
       "      <td>right</td>\n",
       "      <td>The last debate, my opponent said his wife was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>BUSH</td>\n",
       "      <td>2000</td>\n",
       "      <td>rep</td>\n",
       "      <td>right</td>\n",
       "      <td>I don’t think a president can do that. I was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>GORE</td>\n",
       "      <td>2000</td>\n",
       "      <td>dem</td>\n",
       "      <td>left</td>\n",
       "      <td>Well, Jim, the FDA took 12 years, and I do su...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       name  year dem/rep left/right  \\\n",
       "0     PENCE  2020     rep      right   \n",
       "1     PENCE  2020     rep      right   \n",
       "2     PENCE  2020     rep      right   \n",
       "3    HARRIS  2020     dem       left   \n",
       "4     PENCE  2020     rep      right   \n",
       "..      ...   ...     ...        ...   \n",
       "135    BUSH  2004     rep      right   \n",
       "136    BUSH  2004     rep      right   \n",
       "137    BUSH  2004     rep      right   \n",
       "138    BUSH  2000     rep      right   \n",
       "139    GORE  2000     dem       left   \n",
       "\n",
       "                                               content  \n",
       "0     Well thank you for the question, but I’ll use...  \n",
       "1     My hope is that when the hearing takes place,...  \n",
       "2     – treated respectfully and voted and confirme...  \n",
       "3     Thank you, Susan. First of all, Joe Biden and...  \n",
       "4     Well, thank you, Susan. Let me just say, addr...  \n",
       "..                                                 ...  \n",
       "135  Take, for example, the ban on partial birth ab...  \n",
       "136  What I’m saying is, is that as we promote life...  \n",
       "137  The last debate, my opponent said his wife was...  \n",
       "138   I don’t think a president can do that. I was ...  \n",
       "139   Well, Jim, the FDA took 12 years, and I do su...  \n",
       "\n",
       "[140 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140, 5)\n"
     ]
    }
   ],
   "source": [
    "#Scraped Data for Vice Presidential Election 2020 (Kamala Harris and Mike Pence)\n",
    "\n",
    "URL = \"https://debates.org/voter-education/debate-transcripts/vice-presidential-debate-at-the-university-of-utah-in-salt-lake-city-utah/\" \n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "all_paragraphs = soup.find_all('p')\n",
    "\n",
    "# Find the indices of paragraphs containing the word \"abortion\"\n",
    "abortion_indices = [index for index, p in enumerate(all_paragraphs) if 'abortion' in p.get_text().lower()]\n",
    "\n",
    "if abortion_indices:\n",
    "    # Extract paragraphs between the first and last mention of \"abortion\"\n",
    "    start_index = abortion_indices[0]\n",
    "    end_index = abortion_indices[-1]\n",
    "    \n",
    "    paragraphs_between_abortion = all_paragraphs[start_index:end_index + 1]\n",
    "\n",
    "    # Create a DataFrame with columns for 'name', 'dem/rep', and 'content'\n",
    "    data = {'name': [], 'year': [], 'dem/rep': [], 'left/right': [], 'content': []}\n",
    "    count_pence = 0\n",
    "\n",
    "    for paragraph in paragraphs_between_abortion:\n",
    "        count_pence += paragraph.get_text().count('PENCE:')\n",
    "        \n",
    "        if count_pence == 4:\n",
    "            # Modify the paragraph in-place if it's the fourth instance of 'PENCE:'\n",
    "            paragraph.string = re.sub(r'^PENCE:', 'HARRIS:', paragraph.get_text())\n",
    "    \n",
    "        # Skip paragraphs that start with \"PAGE:\"\n",
    "        if not paragraph.get_text().startswith(\"PAGE:\"):\n",
    "            # Determine 'name' and 'dem/rep' based on the speaker\n",
    "            if 'PENCE:' in paragraph.get_text():\n",
    "                data['name'].append('PENCE')\n",
    "                data['year'].append(2020)\n",
    "                data['dem/rep'].append('rep')\n",
    "                data['left/right'].append('right')\n",
    "            else:\n",
    "                data['name'].append('HARRIS')\n",
    "                data['year'].append(2020)\n",
    "                data['dem/rep'].append('dem')\n",
    "                data['left/right'].append('left')\n",
    "            \n",
    "            data['content'].append(re.sub(r'^PENCE:|HARRIS:', '', paragraph.get_text()))\n",
    "\n",
    "    prez_df_2020 = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "#Scraped Data for Vice Presidential Election 2016 (Mike Pence and Tim Kaine)\n",
    "\n",
    "#October 4, 2016\n",
    "\n",
    "URL = \"https://www.debates.org/voter-education/debate-transcripts/october-4-2016-debate-transcript/\"\n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "all_paragraphs = soup.find_all('p')\n",
    "\n",
    "# Find the index of the paragraph with the last mention of \"abortion\"\n",
    "last_abortion_index = max([index for index, p in enumerate(all_paragraphs) if 'abortion' in p.get_text().lower()], default=0)\n",
    "\n",
    "# Find the index of the paragraph starting with \"PENCE: But for me\"\n",
    "pence_index = next((i for i, p in enumerate(all_paragraphs) if p.get_text().startswith(\"PENCE: But for me\")), None)\n",
    "\n",
    "# Extract paragraphs between \"PENCE: But for me\" and the last mention of \"abortion\"\n",
    "paragraphs_selected = all_paragraphs[pence_index:last_abortion_index + 1]\n",
    "\n",
    "# Create a DataFrame with columns for 'name', 'dem/rep', 'left/right', and 'content'\n",
    "data = {'name': [], 'year': [], 'dem/rep': [], 'left/right': [], 'content': []}\n",
    "\n",
    "current_speaker = None\n",
    "\n",
    "for paragraph in paragraphs_selected:\n",
    "    text = paragraph.get_text()\n",
    "    \n",
    "    # Skip paragraphs that start with \"QUIJANO:\"\n",
    "    if text.startswith(\"QUIJANO:\"):\n",
    "        continue\n",
    "    \n",
    "    # Determine 'name' and 'dem/rep' based on the speaker\n",
    "    if 'PENCE:' in text:\n",
    "        current_speaker = 'PENCE'\n",
    "        dem_rep = 'rep'\n",
    "    elif 'KAINE:' in text:\n",
    "        current_speaker = 'KAINE'\n",
    "        dem_rep = 'dem'\n",
    "    \n",
    "    # Determine 'left/right' based on the speaker\n",
    "    if 'PENCE:' in text:\n",
    "        left_right = 'right'\n",
    "    elif 'KAINE:' in text:\n",
    "        left_right = 'left'\n",
    "    \n",
    "    # Append data to the dictionary\n",
    "    data['name'].append(current_speaker)\n",
    "    data['year'].append(2016)\n",
    "    data['dem/rep'].append(dem_rep)\n",
    "    data['left/right'].append(left_right)\n",
    "    data['content'].append(re.sub(r'^PENCE:|KAINE:', '', paragraph.get_text()))\n",
    "    \n",
    "# Create a DataFrame from the dictionary\n",
    "prez_df_2016 = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Scraped Data for Third residential Election 2016 (Hilary Clinton and Donald Trump) \n",
    "\n",
    "#October 19, 2016\n",
    "\n",
    "URL = \"https://www.debates.org/voter-education/debate-transcripts/october-19-2016-debate-transcript/\" \n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "all_paragraphs = soup.find_all('p')\n",
    "\n",
    "# Find the indices of paragraphs containing the word \"abortion\"\n",
    "abortion_indices = [index for index, p in enumerate(all_paragraphs) if 'abortion' in p.get_text().lower()]\n",
    "\n",
    "if abortion_indices:\n",
    "    # Extract paragraphs between the first and last mention of \"abortion\"\n",
    "    start_index = abortion_indices[0]\n",
    "    end_index = abortion_indices[-1]\n",
    "    \n",
    "    paragraphs_between_abortion = all_paragraphs[start_index:end_index + 1]\n",
    "\n",
    "    # Create a DataFrame with columns for 'name', 'year', 'dem/rep', 'left/right', 'content'\n",
    "    data = {'name': [], 'year': [], 'dem/rep': [], 'left/right': [], 'content': []}\n",
    "\n",
    "    current_speaker = None\n",
    "\n",
    "    for paragraph in paragraphs_between_abortion:\n",
    "        # Skip paragraphs that start with \"WALLACE:\"\n",
    "        if not paragraph.get_text().startswith(\"WALLACE:\"):\n",
    "            # Determine 'name' and 'dem/rep' based on the speaker\n",
    "            if 'TRUMP:' in paragraph.get_text():\n",
    "                current_speaker = 'TRUMP'\n",
    "            elif 'CLINTON:' in paragraph.get_text():\n",
    "                current_speaker = 'CLINTON'\n",
    "            \n",
    "            data['name'].append(current_speaker)\n",
    "            data['year'].append(2016)\n",
    "            data['dem/rep'].append('rep' if current_speaker == 'TRUMP' else 'dem')\n",
    "            data['left/right'].append('right' if current_speaker == 'TRUMP' else 'left')\n",
    "            data['content'].append(re.sub(r'^TRUMP:|CLINTON:', '', paragraph.get_text()))\n",
    "\n",
    "    prez_df_2016_2 = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Scraped Data for Vice Presidential Election 2012 (Biden and Ryan)\n",
    "\n",
    "#October 11, 2012\n",
    "\n",
    "URL = \"https://www.debates.org/voter-education/debate-transcripts/october-11-2012-the-biden-romney-vice-presidential-debate/\"\n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "all_paragraphs = soup.find_all('p')\n",
    "\n",
    "# Find the index of the paragraph containing \"RYAN: I don’t see how a\"\n",
    "ryan_paragraph_index = next((index for index, p in enumerate(all_paragraphs) if 'RYAN: I don’t see how a' in p.get_text()), None)\n",
    "\n",
    "# Find the indices of paragraphs containing the word \"abortion\" after the specified paragraph\n",
    "abortion_indices = [index for index, p in enumerate(all_paragraphs[ryan_paragraph_index:]) if 'abortion' in p.get_text().lower()]\n",
    "\n",
    "if abortion_indices:\n",
    "    # Adjust the start index to include the paragraph containing \"RYAN: I don’t see how a\"\n",
    "    start_index = ryan_paragraph_index\n",
    "    end_index = ryan_paragraph_index + abortion_indices[-1]\n",
    "    \n",
    "    paragraphs_between_abortion = all_paragraphs[start_index:end_index + 1]\n",
    "\n",
    "    # Create a DataFrame with columns for 'name', 'year', 'dem/rep', 'left/right', 'content'\n",
    "    data = {'name': [], 'year': [], 'dem/rep': [], 'left/right': [], 'content': []}\n",
    "\n",
    "    current_speaker = None\n",
    "\n",
    "    for paragraph in paragraphs_between_abortion:\n",
    "        # Skip paragraphs that start with \"RADDATZ:\"\n",
    "        if not paragraph.get_text().startswith(\"RADDATZ:\"):\n",
    "            # Determine 'name' and 'dem/rep' based on the speaker\n",
    "            if 'RYAN:' in paragraph.get_text():\n",
    "                current_speaker = 'RYAN'\n",
    "            elif 'BIDEN:' in paragraph.get_text():\n",
    "                current_speaker = 'BIDEN'\n",
    "            \n",
    "            data['name'].append(current_speaker)\n",
    "            data['year'].append(2012)\n",
    "            data['dem/rep'].append('rep' if current_speaker == 'RYAN' else 'dem')\n",
    "            data['left/right'].append('right' if current_speaker == 'RYAN' else 'left')\n",
    "            data['content'].append(re.sub(r'^RYAN:|BIDEN:', '', paragraph.get_text()))\n",
    "\n",
    "    prez_df_2012 = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "#Scraped Data for Presidential Election 2008 (Third McCain and Obama Debate)\n",
    "\n",
    "#October 15, 2008\n",
    "\n",
    "URL = \"https://www.debates.org/voter-education/debate-transcripts/october-15-2008-debate-transcript/\" \n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "all_paragraphs = soup.find_all('p')\n",
    "\n",
    "# Find the indices of paragraphs containing the word \"abortion\"\n",
    "abortion_indices = [index for index, p in enumerate(all_paragraphs) if 'abortion' in p.get_text().lower()]\n",
    "\n",
    "if abortion_indices:\n",
    "    # Extract paragraphs between the first and last mention of \"abortion\"\n",
    "    start_index = abortion_indices[0]\n",
    "    end_index = abortion_indices[-1]\n",
    "    \n",
    "    paragraphs_between_abortion = all_paragraphs[start_index:end_index + 1]\n",
    "\n",
    "    # Create a DataFrame with columns for 'name', 'year', 'dem/rep', 'left/right', 'content'\n",
    "    data = {'name': [], 'year': [], 'dem/rep': [], 'left/right': [], 'content': []}\n",
    "\n",
    "    current_speaker = None\n",
    "\n",
    "    for paragraph in paragraphs_between_abortion:\n",
    "        # Skip paragraphs that start with \"SCHIEFFER::\"\n",
    "        if not paragraph.get_text().startswith(\"SCHIEFFER:\"):\n",
    "            # Determine 'name' and 'dem/rep' based on the speaker\n",
    "            if 'MCCAIN:' in paragraph.get_text():\n",
    "                current_speaker = 'MCCAIN'\n",
    "            elif 'OBAMA:' in paragraph.get_text():\n",
    "                current_speaker = 'OBAMA'\n",
    "            \n",
    "            data['name'].append(current_speaker)\n",
    "            data['year'].append(2008)\n",
    "            data['dem/rep'].append('rep' if current_speaker == 'MCCAIN' else 'dem')\n",
    "            data['left/right'].append('right' if current_speaker == 'MCCAIN' else 'left')\n",
    "            data['content'].append(re.sub(r'^MCCAIN:|OBAMA:', '', paragraph.get_text()))\n",
    "\n",
    "    prez_df_2008 = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "#Scraped Data for Presidential Election 2004 (Bush and Kerry)\n",
    "\n",
    "#October 13, 2004\n",
    "\n",
    "URL = \"https://www.debates.org/voter-education/debate-transcripts/october-13-2004-debate-transcript/\" \n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "all_paragraphs = soup.find_all('p')\n",
    "\n",
    "# Find the index of the paragraph containing \"RYAN: I don’t see how a\"\n",
    "kerry_paragraph_index = next((index for index, p in enumerate(all_paragraphs) if 'KERRY: I respect their views.' in p.get_text()), None)\n",
    "\n",
    "# Find the indices of paragraphs containing the word \"abortion\" after the specified paragraph\n",
    "abortion_indices = [index for index, p in enumerate(all_paragraphs[kerry_paragraph_index:]) if 'abortion' in p.get_text().lower()]\n",
    "\n",
    "if abortion_indices:\n",
    "    # Adjust the start index to include the paragraph containing \"KERRY: I respect their views.\"\n",
    "    start_index = kerry_paragraph_index\n",
    "    end_index = kerry_paragraph_index + abortion_indices[-1]\n",
    "    \n",
    "    paragraphs_between_abortion = all_paragraphs[start_index:end_index + 1]\n",
    "\n",
    "    # Create a DataFrame with columns for 'name', 'year', 'dem/rep', 'left/right', 'content'\n",
    "    data = {'name': [], 'year': [], 'dem/rep': [], 'left/right': [], 'content': []}\n",
    "\n",
    "    current_speaker = None\n",
    "\n",
    "    for paragraph in paragraphs_between_abortion:\n",
    "        # Skip paragraphs that start with \"SCHIEFFER\"\n",
    "        if not paragraph.get_text().startswith(\"SCHIEFFER\"):\n",
    "            # Determine 'name' and 'dem/rep' based on the speaker\n",
    "            if 'BUSH:' in paragraph.get_text():\n",
    "                current_speaker = 'BUSH'\n",
    "            elif 'KERRY:' in paragraph.get_text():\n",
    "                current_speaker = 'KERRY'\n",
    "            \n",
    "            data['name'].append(current_speaker)\n",
    "            data['year'].append(2004)\n",
    "            data['dem/rep'].append('rep' if current_speaker == 'BUSH' else 'dem')\n",
    "            data['left/right'].append('right' if current_speaker == 'BUSH' else 'left')\n",
    "            data['content'].append(re.sub(r'^BUSH:|KERRY:', '', paragraph.get_text()))\n",
    "\n",
    "    prez_df_2004 = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Scraped Data for Presidential Election 2004 (Bush and Kerry)\n",
    "\n",
    "#October 8, 2004\n",
    "\n",
    "URL = \"https://www.debates.org/voter-education/debate-transcripts/october-8-2004-debate-transcript/\" \n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "all_paragraphs = soup.find_all('p')\n",
    "\n",
    "# Find the index of the paragraph containing \"RYAN: I don’t see how a\"\n",
    "starting_index = next((index for index, p in enumerate(all_paragraphs) if 'DEGENHART: Senator Kerry, suppose' in p.get_text()), None)\n",
    "\n",
    "# Find the indices of paragraphs containing the word \"abortion\" after the specified paragraph\n",
    "abortion_indices = [index for index, p in enumerate(all_paragraphs[starting_index:]) if 'abortion' in p.get_text().lower()]\n",
    "\n",
    "if abortion_indices:\n",
    "    end_index = starting_index + abortion_indices[-1]\n",
    "    \n",
    "    paragraphs_between_abortion = all_paragraphs[starting_index:end_index + 1]\n",
    "\n",
    "    # Create a DataFrame with columns for 'name', 'year', 'dem/rep', 'left/right', 'content'\n",
    "    data = {'name': [], 'year': [], 'dem/rep': [], 'left/right': [], 'content': []}\n",
    "\n",
    "    current_speaker = None\n",
    "\n",
    "    for paragraph in paragraphs_between_abortion:\n",
    "        # Skip paragraphs that start with \"DEGENHART:\"\n",
    "        if not paragraph.get_text().startswith(\"DEGENHART:\") and not paragraph.get_text().startswith(\"GIBSON:\"):\n",
    "            # Determine 'name' and 'dem/rep' based on the speaker\n",
    "            if 'BUSH:' in paragraph.get_text():\n",
    "                current_speaker = 'BUSH'\n",
    "            elif 'KERRY:' in paragraph.get_text():\n",
    "                current_speaker = 'KERRY'\n",
    "            \n",
    "            data['name'].append(current_speaker)\n",
    "            data['year'].append(2004)\n",
    "            data['dem/rep'].append('rep' if current_speaker == 'BUSH' else 'dem')\n",
    "            data['left/right'].append('right' if current_speaker == 'BUSH' else 'left')\n",
    "            data['content'].append(re.sub(r'^BUSH:|KERRY:', '', paragraph.get_text()))\n",
    "\n",
    "    prez_df_2004_2 = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "#Scraped Data for Presidential Election 2000 (Gore and Bush)\n",
    "\n",
    "#October 8, 2004\n",
    "\n",
    "URL = \"https://www.debates.org/voter-education/debate-transcripts/october-3-2000-transcript/\" \n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "all_paragraphs = soup.find_all('p')\n",
    "\n",
    "# Find the indices of paragraphs containing the word \"abortion\"\n",
    "abortion_indices = [index for index, p in enumerate(all_paragraphs) if 'abortion' in p.get_text().lower()]\n",
    "\n",
    "if abortion_indices:\n",
    "    # Extract paragraphs between the first and last mention of \"abortion\"\n",
    "    start_index = abortion_indices[0]\n",
    "    end_index = abortion_indices[-1]\n",
    "    \n",
    "    paragraphs_between_abortion = all_paragraphs[start_index:end_index + 1]\n",
    "\n",
    "    # Create a DataFrame with columns for 'name', 'year', 'dem/rep', 'left/right', 'content'\n",
    "    data = {'name': [], 'year': [], 'dem/rep': [], 'left/right': [], 'content': []}\n",
    "\n",
    "    current_speaker = None\n",
    "\n",
    "    for paragraph in paragraphs_between_abortion:\n",
    "        # Skip paragraphs that start with \"MODERATOR::\"\n",
    "        if not paragraph.get_text().startswith(\"MODERATOR:\"):\n",
    "            # Determine 'name' and 'dem/rep' based on the speaker\n",
    "            if 'BUSH:' in paragraph.get_text():\n",
    "                current_speaker = 'BUSH'\n",
    "            elif 'GORE:' in paragraph.get_text():\n",
    "                current_speaker = 'GORE'\n",
    "            \n",
    "            data['name'].append(current_speaker)\n",
    "            data['year'].append(2000)\n",
    "            data['dem/rep'].append('rep' if current_speaker == 'BUSH' else 'dem')\n",
    "            data['left/right'].append('right' if current_speaker == 'BUSH' else 'left')\n",
    "            data['content'].append(re.sub(r'^BUSH:|GORE:', '', paragraph.get_text()))\n",
    "\n",
    "    prez_df_2000 = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "prez_merged = pd.concat([prez_df_2020, prez_df_2016, \n",
    "                         prez_df_2016_2, prez_df_2012, \n",
    "                         prez_df_2008, prez_df_2004_2, \n",
    "                         prez_df_2004, prez_df_2000], ignore_index=True)\n",
    "display(prez_merged)\n",
    "print(prez_merged.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Congressional Bills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_combine_html(urls):\n",
    "    # Initialize an empty string to store the combined HTML content\n",
    "    combined_html_content = ''\n",
    "\n",
    "    # Iterate through each URL in the list\n",
    "    for url in urls:\n",
    "        # Fetch the HTML content from the current URL\n",
    "        response = requests.get(url)\n",
    "        html_content = response.content\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'xml')\n",
    "\n",
    "        # Extract text content from the HTML\n",
    "        text_content = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "        # Append the extracted text content to the result\n",
    "        combined_html_content += text_content + ' '\n",
    "\n",
    "    return combined_html_content\n",
    "\n",
    "#Republican\n",
    "rep_congress_urls = [\n",
    "    \"https://www.congress.gov/118/bills/hr106/BILLS-118hr106ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr7/BILLS-118hr7ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr862/BILLS-118hr862ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr1143/BILLS-118hr1143ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr792/BILLS-118hr792ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr330/BILLS-118hr330ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr1470/BILLS-118hr1470ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr384/BILLS-118hr384ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr26/BILLS-118hr26pcs.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr632/BILLS-118hr632ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr1297/BILLS-118hr1297ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr73/BILLS-118hr73ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr416/BILLS-118hr416ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr383/BILLS-118hr383ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr5806/BILLS-118hr5806ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr5319/BILLS-118hr5319ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr6459/BILLS-118hr6459ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr6460/BILLS-118hr6460ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr3741/BILLS-118hr3741ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr4672/BILLS-118hr4672ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr983/BILLS-118hr983ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr3741/BILLS-118hr3741ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr421/BILLS-118hr421ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr435/BILLS-118hr435ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr175/BILLS-118hr175ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr116/BILLS-118hr116ih.xml\"\n",
    "    # Add more HTML URLs as needed\n",
    "]\n",
    "\n",
    "rep_congress_content = scrape_and_combine_html(rep_congress_urls )\n",
    "\n",
    "\n",
    "#Democratic\n",
    "dem_congress_urls = [\n",
    "    \"https://www.congress.gov/118/bills/hr767/BILLS-118hr767ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr2573/BILLS-118hr2573ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr55/BILLS-118hr55ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr4303/BILLS-118hr4303ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr1723/BILLS-118hr1723ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr561/BILLS-118hr561ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr3132/BILLS-118hr3132ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr12/BILLS-118hr12ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr782/BILLS-118hr782ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr1224/BILLS-118hr1224ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr2736/BILLS-118hr2736ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr6357/BILLS-118hr6357ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr4796/BILLS-118hr4796ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr4147/BILLS-118hr4147ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr4268/BILLS-118hr4268ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr4418/BILLS-118hr4418ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr2907/BILLS-118hr2907ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr445/BILLS-118hr445ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr459/BILLS-118hr459ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr4281/BILLS-118hr4281ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr5008/BILLS-118hr5008ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr3659/BILLS-118hr3659ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr3421/BILLS-118hr3421ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr6298/BILLS-118hr6298ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr4901/BILLS-118hr4901ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr3420/BILLS-118hr3420ih.xml\", \n",
    "    \"https://www.congress.gov/118/bills/hr4121/BILLS-118hr4121ih.xml\",  \n",
    "    \"https://www.congress.gov/118/bills/hr62/BILLS-118hr62ih.xml\",\n",
    "    \"https://www.congress.gov/118/bills/hr6298/BILLS-118hr6298ih.xml\", \n",
    "    \"https://www.congress.gov/118/bills/hr6270/BILLS-118hr6270ih.xml\", \n",
    "    \"https://www.congress.gov/118/bills/hr4329/BILLS-118hr4329ih.xml\"\n",
    "\n",
    "    # Add more HTML URLs as needed\n",
    "]\n",
    "\n",
    "dem_congress_content= scrape_and_combine_html(dem_congress_urls )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(content, custom_stopwords, top_n=10):\n",
    "    # Create a CountVectorizer\n",
    "    vectorizer = CountVectorizer(\n",
    "        stop_words=stopwords.words('english') + custom_stopwords\n",
    "    )\n",
    "\n",
    "    # Fit and transform the content\n",
    "    word_matrix = vectorizer.fit_transform([content])\n",
    "\n",
    "    # Get feature names (words)\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Get word counts\n",
    "    word_counts = word_matrix.toarray()[0]\n",
    "\n",
    "    # Create a DataFrame with words and counts\n",
    "    word_df = pd.DataFrame({'word': words, 'count': word_counts})\n",
    "\n",
    "    # Sort DataFrame by count in descending order\n",
    "    word_df = word_df.sort_values(by='count', ascending=False)\n",
    "\n",
    "    return word_df.head(top_n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>health</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>abortions</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>child</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>united</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>woman</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>physical</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>federal</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>pregnancy</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>public</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>performed</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  count\n",
       "786      health    101\n",
       "152   abortions     97\n",
       "371       child     89\n",
       "1588     united     89\n",
       "1646      woman     66\n",
       "1157   physical     65\n",
       "691     federal     62\n",
       "1199  pregnancy     58\n",
       "1270     public     57\n",
       "1142  performed     55"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom stopwords for Republicans\n",
    "rep_custom_stopwords = ['mr', 'abortion', 'section', 'title', 'act', \n",
    "                        'subsections', 'shall', 'following', 'code', \n",
    "                        '2023','subsection', 'mrs', 'may', 'house', \n",
    "                        'representatives', 'amended', 'bill', 'term',\n",
    "                        'state', 'states', 'minor', 'means']\n",
    "\n",
    "# Call the function for Republicans\n",
    "congress_rep_top = get_top_words(rep_congress_content, rep_custom_stopwords)\n",
    "\n",
    "congress_rep_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2094</th>\n",
       "      <td>health</td>\n",
       "      <td>1133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>care</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3685</th>\n",
       "      <td>services</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>abortion</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3492</th>\n",
       "      <td>reproductive</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270</th>\n",
       "      <td>provider</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>individual</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4140</th>\n",
       "      <td>united</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2265</th>\n",
       "      <td>individuals</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>entity</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              word  count\n",
       "2094        health   1133\n",
       "925           care    840\n",
       "3685      services    665\n",
       "453       abortion    514\n",
       "3492  reproductive    315\n",
       "3270      provider    265\n",
       "2262    individual    242\n",
       "4140        united    240\n",
       "2265   individuals    223\n",
       "1715        entity    216"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom stop words\n",
    "dem_custom_stopwords = ['mr', 'ms', 'including', 'act', 'secretary', \n",
    "                        'title', 'section', 'subsection', 'shall', '42',\n",
    "                        'paragraph', 'described', 'may', 'information',\n",
    "                        'state', 'states']\n",
    "\n",
    "# Call the function for Democrats\n",
    "congress_dem_top = get_top_words(dem_congress_content, dem_custom_stopwords)\n",
    "\n",
    "congress_dem_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>women</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>right</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>faith</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>life</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>court</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>issue</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>woman</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>catholic</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>choice</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>decision</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  count\n",
       "689     women     22\n",
       "534     right     16\n",
       "233     faith     15\n",
       "358      life     15\n",
       "158     court     12\n",
       "322     issue     12\n",
       "688     woman     12\n",
       "109  catholic     12\n",
       "116    choice     11\n",
       "168  decision     10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define custom stopwords for Democrats\n",
    "dem_stopwords = ['president', 'think', 'people', 'wade', 'said', \n",
    "                 'make', 'would', 'know', 'abortion', 'one', 'let',\n",
    "                 'going', 'roe']\n",
    "\n",
    "# Filter rows where 'dem/rep' is 'dem'\n",
    "dem_df = prez_merged[prez_merged['dem/rep'] == 'dem']\n",
    "\n",
    "# Combine all content into a single string\n",
    "left_content = ' '.join(dem_df['content'])\n",
    "\n",
    "# Call the function for Democrats\n",
    "prez_dem_top = get_top_words(left_content, dem_stopwords)\n",
    "\n",
    "prez_dem_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>life</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>america</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>pro</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>court</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>birth</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>promote</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>voted</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>american</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>law</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>child</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  count\n",
       "306      life     45\n",
       "31    america     16\n",
       "415       pro     15\n",
       "133     court     12\n",
       "77      birth     10\n",
       "420   promote     10\n",
       "576     voted     10\n",
       "32   american      9\n",
       "297       law      9\n",
       "99      child      9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define custom stopwords for Republicans\n",
    "rep_stopwords = ['abortion', 'know', 'think', 'would', 'people', \n",
    "                 'senator', 'abortions', 'one', 'say', 'really',\n",
    "                 'way', 'going', 'states']\n",
    "\n",
    "\n",
    "# Filter rows where 'dem/rep' is 'rep'\n",
    "rep_df = prez_merged[prez_merged['dem/rep'] == 'rep']\n",
    "\n",
    "# Combine all content into a single string\n",
    "right_content = ' '.join(rep_df['content'])\n",
    "\n",
    "# Call the function for Republicans\n",
    "prez_rep_top = get_top_words(right_content, rep_stopwords)\n",
    "\n",
    "prez_rep_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LABEL CREATION FROM OVERLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RIGHT LABELS\n",
    "\n",
    "# Extract sets of unique words from DataFrames\n",
    "congress_rep_set = set(congress_rep_top['word'])\n",
    "prez_rep_set = set(prez_rep_top['word'])\n",
    "\n",
    "# Combine the sets into a single set of labels\n",
    "combined_right = congress_rep_set.union(prez_rep_set)\n",
    "\n",
    "# Print the combined set of labels\n",
    "# print(combined_right)\n",
    "\n",
    "len(combined_right)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LEFT LABELS\n",
    "\n",
    "# Extract sets of unique words from DataFrames\n",
    "congress_dem_set = set(congress_dem_top['word'])\n",
    "prez_dem_set = set(prez_dem_top['word'])\n",
    "\n",
    "# Combine the sets into a single set of labels\n",
    "combined_left = congress_dem_set.union(prez_dem_set)\n",
    "\n",
    "# Print the combined set of labels\n",
    "# print(combined_left)\n",
    "\n",
    "len(combined_left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Left and Right Labels Within All News Outlets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape in all News Outlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt_to_dataframe(file_path):\n",
    "    # Initialize empty lists to store data\n",
    "    news_outlets = []\n",
    "    titles = []\n",
    "    authors = []\n",
    "    publication_dates = []\n",
    "    article_contents = []\n",
    "\n",
    "    # Open the text file\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        # Initialize variables to store information\n",
    "        current_article = {}\n",
    "        \n",
    "        # Initialize list to store dictionaries\n",
    "        articles_data = []\n",
    "        \n",
    "        # Iterate through lines in the file\n",
    "        for line in lines:\n",
    "            # Split the line into key and value if possible\n",
    "            line_parts = line.split(':', 1)\n",
    "            \n",
    "            # Check if the line can be split into key and value\n",
    "            if len(line_parts) == 2:\n",
    "                key, value = map(str.strip, line_parts)\n",
    "                \n",
    "                # Check for the end of an article\n",
    "                if key == 'Article_Content':\n",
    "                    # Save the current article information\n",
    "                    current_article['Article_Content'] = value.strip()\n",
    "                    \n",
    "                    articles_data.append({\n",
    "                        'News_Outlet': current_article.get('News_Outlet', ''),\n",
    "                        'Title': current_article.get('Title', ''),\n",
    "                        'Author': current_article.get('Author', ''),\n",
    "                        'Publication_Date': current_article.get('Publication_Date', ''),\n",
    "                        'Article_Content': current_article.get('Article_Content', '')\n",
    "                    })\n",
    "\n",
    "                    current_article = {}\n",
    "                else:\n",
    "                    # Add key-value pair to current article dictionary\n",
    "                    current_article[key] = value\n",
    "\n",
    "    df = pd.DataFrame(articles_data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "cnn_data_frame = read_txt_to_dataframe('CNN.txt')\n",
    "cnn_data_frame = cnn_data_frame.reset_index()\n",
    "cnn_data_frame = cnn_data_frame.rename(columns={'index': 'docid'})\n",
    "print(len(cnn_data_frame))\n",
    "\n",
    "nyt_data_frame = read_txt_to_dataframe('NYT.txt')\n",
    "nyt_data_frame = nyt_data_frame.reset_index()\n",
    "nyt_data_frame = nyt_data_frame.rename(columns={'index': 'docid'})\n",
    "print(len(nyt_data_frame))\n",
    "\n",
    "fox_data_frame = read_txt_to_dataframe('FOX.txt')\n",
    "fox_data_frame = fox_data_frame.reset_index()\n",
    "fox_data_frame = fox_data_frame.rename(columns={'index': 'docid'})\n",
    "print(len(fox_data_frame))\n",
    "\n",
    "wsj_data_frame = read_txt_to_dataframe('WSJ.txt')\n",
    "wsj_data_frame = wsj_data_frame.reset_index()\n",
    "wsj_data_frame = wsj_data_frame.rename(columns={'index': 'docid'})\n",
    "print(len(wsj_data_frame))\n",
    "\n",
    "nyp_data_frame = read_txt_to_dataframe('NYP.txt')\n",
    "nypdata_frame = nyp_data_frame.reset_index()\n",
    "nyp_data_frame = nyp_data_frame.rename(columns={'index': 'docid'})\n",
    "print(len(nyp_data_frame))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_outlets = pd.concat([cnn_data_frame, \n",
    "                              nyt_data_frame, \n",
    "                              fox_data_frame, \n",
    "                              wsj_data_frame, \n",
    "                              nyp_data_frame], \n",
    "                              ignore_index=True)\n",
    "\n",
    "merged_content = merged_outlets['Article_Content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n",
      "102\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "# Create a new column 'binary_label' for classification (Left, Right, Neutral)\n",
    "merged_outlets['party_label'] = 'Neutral'  # Default label\n",
    "\n",
    "# Iterate over rows and label based on the distribution of words\n",
    "for index, row in merged_outlets.iterrows():\n",
    "    left_count = sum(word in row['Article_Content'] for word in combined_left)\n",
    "    right_count = sum(word in row['Article_Content'] for word in combined_right)\n",
    "\n",
    "    if left_count > right_count:\n",
    "        merged_outlets.at[index, 'party_label'] = 'Left'\n",
    "    elif right_count > left_count:\n",
    "        merged_outlets.at[index, 'party_label'] = 'Right'\n",
    "\n",
    "print(len(merged_outlets[merged_outlets['party_label'] == 'Neutral']))\n",
    "print(len(merged_outlets[merged_outlets['party_label'] == 'Left']))\n",
    "print(len(merged_outlets[merged_outlets['party_label'] == 'Right']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Baseline Accuracy Score:  0.42\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.41      0.85      0.56        20\n",
      "     Neutral       0.44      0.27      0.33        15\n",
      "       Right       0.00      0.00      0.00        15\n",
      "\n",
      "    accuracy                           0.42        50\n",
      "   macro avg       0.29      0.37      0.30        50\n",
      "weighted avg       0.30      0.42      0.32        50\n",
      "\n",
      "Cross-validation score of Logistic Regression:  0.4155761492309867 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Baseline Accuracy Score:  0.54\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Left       0.54      0.75      0.63        20\n",
      "     Neutral       0.75      0.20      0.32        15\n",
      "       Right       0.50      0.60      0.55        15\n",
      "\n",
      "    accuracy                           0.54        50\n",
      "   macro avg       0.60      0.52      0.50        50\n",
      "weighted avg       0.59      0.54      0.51        50\n",
      "\n",
      "Cross validation score of Random Forest:  0.5355208992895877\n"
     ]
    }
   ],
   "source": [
    "# Stratify and Split Raw Data into Train and Test groups\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    merged_outlets['Article_Content'], merged_outlets['party_label'], \n",
    "    test_size=0.2, random_state=None,\n",
    "    stratify=merged_outlets['party_label'])\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(train_texts)\n",
    "test_vectors = vectorizer.transform(test_texts)\n",
    "\n",
    "# Feature selection with SelectKBest and f_classif\n",
    "k_best = 20  # Adjust as needed\n",
    "selector = SelectKBest(f_classif, k=k_best)\n",
    "train_vectors_selected = selector.fit_transform(train_vectors, train_labels)\n",
    "test_vectors_selected = selector.transform(test_vectors)\n",
    "\n",
    "# Convert sparse matrix to dense array\n",
    "train_vectors_dense = train_vectors_selected.toarray()\n",
    "test_vectors_dense = test_vectors_selected.toarray()\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "lg_class = LogisticRegression(max_iter=1000)\n",
    "log_reg = lg_class.fit(train_vectors_dense, train_labels)\n",
    "log_predictions = log_reg.predict(test_vectors_dense)\n",
    "\n",
    "# Print classification report\n",
    "print('Logistic Regression Baseline Accuracy Score: ', accuracy_score(test_labels, log_predictions))\n",
    "print(classification_report(test_labels, log_predictions, zero_division=0))\n",
    "\n",
    "# Calculate and Print Cross Validated Score for SelectKBest of Logistic Regression\n",
    "lg_cv_score = cross_val_score(lg_class, train_vectors_dense, train_labels, cv=5, scoring='f1_weighted').mean()\n",
    "print(\"Cross-validation score of Logistic Regression: \", lg_cv_score, '\\n\\n')\n",
    "\n",
    "# Random Forest Baseline\n",
    "rf_class = RandomForestClassifier()\n",
    "ran_forest = rf_class.fit(train_vectors_dense, train_labels)\n",
    "ran_forest_predict = ran_forest.predict(test_vectors_dense)\n",
    "print('Random Forest Baseline Accuracy Score: ', accuracy_score(test_labels, ran_forest_predict))\n",
    "print(classification_report(test_labels, ran_forest_predict))\n",
    "\n",
    "# Calculate and Print Cross Validated Score for Random Forest\n",
    "rf_cv_score = cross_val_score(rf_class, train_vectors_dense, train_labels, cv=5, scoring='f1_weighted').mean()\n",
    "print(\"Cross validation score of Random Forest: \", rf_cv_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Design, implement, and evaluate the performance of a classifier based on \n",
    "#(Distil)BERT or other large language model. Measure performance using one or \n",
    "#more metrics directly comparable to the one(s) used for your baseline method.\n",
    "\n",
    "max_length = 512  # Set the maximum length for tokenization\n",
    "\n",
    "# Tokenize the poems using DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 3  \n",
    "\n",
    "# Initialize StratifiedKFold for cross-validation\n",
    "skf = StratifiedKFold(n_splits=n_splits, random_state=None, shuffle=True)\n",
    "\n",
    "# Custom dataset class for PyTorch, handling encodings and labels\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# # Training arguments for the Hugging Face Transformers library\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',         # Outputs directory\n",
    "#     logging_dir='./logs',           # Directory for storing logs\n",
    "#     per_device_train_batch_size=9,  # Set the batch size per device for training\n",
    "#     per_device_eval_batch_size=8,   # Set the batch size for evaluation\n",
    "#     num_train_epochs=4,             # Total Number of training epochs\n",
    "#     learning_rate=1e-4,             # Initial learning rate for Adam optimizer\n",
    "#     warmup_steps=4,                 # Number of warm-up steps for learning rate scheduler\n",
    "#     weight_decay=0.005,             # Weight decay strength\n",
    "#     logging_steps=6,               # Logging steps within directory during training\n",
    "#     evaluation_strategy='steps'     # Evaluate steps during fine-tuning to see progress\n",
    "# )\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=10,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    learning_rate=2e-5,              # initial learning rate for Adam optimizer\n",
    "    warmup_steps=200,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
    "    weight_decay=0.1,               # strength of weight decay\n",
    "    output_dir='./results',          # output directory\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=100,               # number of steps to output logging (set lower because of small dataset size)\n",
    "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
    ")\n",
    "\n",
    "# Function to compute evaluation metrics for model predictions\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    score1 = accuracy_score(labels, preds)\n",
    "    score = f1_score(labels, preds, average='weighted')\n",
    "    return {'f1': score, 'accuracy': score1}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "Lengths of train and tests texts and labels: \n",
      "Train texts:  200\n",
      "Train labels:  200\n",
      "Test texts:  50\n",
      "Test labels:  50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2c1a2f42024cc99449449a468a72cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0815, 'learning_rate': 1e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80359234a92c496c89dca2971f909709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.050831913948059, 'eval_f1': 0.4309441570965197, 'eval_accuracy': 0.5, 'eval_runtime': 12.4303, 'eval_samples_per_second': 4.022, 'eval_steps_per_second': 0.563, 'epoch': 5.0}\n",
      "{'train_runtime': 905.2465, 'train_samples_per_second': 1.105, 'train_steps_per_second': 0.11, 'train_loss': 1.081508026123047, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd90f2d2d79344a2aa4cac3ccb2927f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3e3f2c35454b8b97ea8232ce1b24b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Predicted Labels:  50\n",
      "Printed Predicted Labels:  ['Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Neutral', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left']\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.07      0.12        14\n",
      "           1       0.71      0.33      0.45        15\n",
      "           2       0.47      0.90      0.62        21\n",
      "\n",
      "    accuracy                           0.50        50\n",
      "   macro avg       0.51      0.44      0.40        50\n",
      "weighted avg       0.51      0.50      0.43        50\n",
      "\n",
      "Fold 1:\n",
      "Lengths of train and tests texts and labels: \n",
      "Train texts:  200\n",
      "Train labels:  200\n",
      "Test texts:  50\n",
      "Test labels:  50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616c69cd85984e8999777d1587fc1a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0791, 'learning_rate': 1e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e4e33cfc4842e58cee445a96b4a317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0718011856079102, 'eval_f1': 0.22352941176470587, 'eval_accuracy': 0.38, 'eval_runtime': 10.9939, 'eval_samples_per_second': 4.548, 'eval_steps_per_second': 0.637, 'epoch': 5.0}\n",
      "{'train_runtime': 997.2642, 'train_samples_per_second': 1.003, 'train_steps_per_second': 0.1, 'train_loss': 1.0790774536132812, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a749a6d7e1640b987784b6a5a59feb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60da04f2c4244f785baaa60460180fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Predicted Labels:  50\n",
      "Printed Predicted Labels:  ['Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left', 'Left']\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        15\n",
      "           1       0.00      0.00      0.00        15\n",
      "           2       0.40      0.95      0.56        20\n",
      "\n",
      "    accuracy                           0.38        50\n",
      "   macro avg       0.13      0.32      0.19        50\n",
      "weighted avg       0.16      0.38      0.22        50\n",
      "\n",
      "Fold 2:\n",
      "Lengths of train and tests texts and labels: \n",
      "Train texts:  200\n",
      "Train labels:  200\n",
      "Test texts:  50\n",
      "Test labels:  50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/3350 /lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Applications/anaconda3/envs/3350 /lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Applications/anaconda3/envs/3350 /lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4e9bbabf59496f89d0f091b1652e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jc/INFO 3350/info3350-final/bert.ipynb Cell 27\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/INFO%203350/info3350-final/bert.ipynb#X41sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m model_fold \u001b[39m=\u001b[39m DistilBertForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mdistilbert-base-cased\u001b[39m\u001b[39m'\u001b[39m, num_labels\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(id2label))\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/INFO%203350/info3350-final/bert.ipynb#X41sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m trainer_fold \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/INFO%203350/info3350-final/bert.ipynb#X41sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel_fold,                    \u001b[39m# instantiated Transformers model to be trained\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/INFO%203350/info3350-final/bert.ipynb#X41sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,                  \u001b[39m# Training arguments initialized above\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/INFO%203350/info3350-final/bert.ipynb#X41sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics      \u001b[39m# Custom evaluation function\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/INFO%203350/info3350-final/bert.ipynb#X41sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jc/INFO%203350/info3350-final/bert.ipynb#X41sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m trainer_fold\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/INFO%203350/info3350-final/bert.ipynb#X41sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m trainer_fold\u001b[39m.\u001b[39mevaluate()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/INFO%203350/info3350-final/bert.ipynb#X41sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# Get predictions on the test set\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/3350 /lib/python3.11/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1556\u001b[0m         args\u001b[39m=\u001b[39margs,\n\u001b[1;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39mtrial,\n\u001b[1;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1560\u001b[0m     )\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/3350 /lib/python3.11/site-packages/transformers/trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1859\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1860\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1862\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/3350 /lib/python3.11/site-packages/transformers/trainer.py:2725\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2722\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2724\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2725\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   2727\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2728\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/3350 /lib/python3.11/site-packages/transformers/trainer.py:2748\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2746\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2747\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2748\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[1;32m   2749\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2750\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2751\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/3350 /lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/3350 /lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:779\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    773\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    776\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 779\u001b[0m distilbert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistilbert(\n\u001b[1;32m    780\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    781\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m    782\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[1;32m    783\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m    784\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    785\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    786\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    787\u001b[0m )\n\u001b[1;32m    788\u001b[0m hidden_state \u001b[39m=\u001b[39m distilbert_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    789\u001b[0m pooled_output \u001b[39m=\u001b[39m hidden_state[:, \u001b[39m0\u001b[39m]  \u001b[39m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/3350 /lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/3350 /lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:599\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    595\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    597\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids, inputs_embeds)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 599\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(\n\u001b[1;32m    600\u001b[0m     x\u001b[39m=\u001b[39membeddings,\n\u001b[1;32m    601\u001b[0m     attn_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[1;32m    602\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[1;32m    603\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    604\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    605\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    606\u001b[0m )\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/3350 /lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/3350 /lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:369\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    361\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    362\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    363\u001b[0m         hidden_state,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m         output_attentions,\n\u001b[1;32m    367\u001b[0m     )\n\u001b[1;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 369\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    370\u001b[0m         hidden_state,\n\u001b[1;32m    371\u001b[0m         attn_mask,\n\u001b[1;32m    372\u001b[0m         head_mask[i],\n\u001b[1;32m    373\u001b[0m         output_attentions,\n\u001b[1;32m    374\u001b[0m     )\n\u001b[1;32m    376\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    378\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/3350 /lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/3350 /lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:295\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[39m# Self-Attention\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(\n\u001b[1;32m    296\u001b[0m     query\u001b[39m=\u001b[39mx,\n\u001b[1;32m    297\u001b[0m     key\u001b[39m=\u001b[39mx,\n\u001b[1;32m    298\u001b[0m     value\u001b[39m=\u001b[39mx,\n\u001b[1;32m    299\u001b[0m     mask\u001b[39m=\u001b[39mattn_mask,\n\u001b[1;32m    300\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[1;32m    301\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    302\u001b[0m )\n\u001b[1;32m    303\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    304\u001b[0m     sa_output, sa_weights \u001b[39m=\u001b[39m sa_output  \u001b[39m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/3350 /lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/3350 /lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:227\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    222\u001b[0m scores \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39mmasked_fill(\n\u001b[1;32m    223\u001b[0m     mask, torch\u001b[39m.\u001b[39mtensor(torch\u001b[39m.\u001b[39mfinfo(scores\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mmin)\n\u001b[1;32m    224\u001b[0m )  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[1;32m    226\u001b[0m weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(weights)  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[39m# Mask heads if we want to\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/3350 /lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/3350 /lib/python3.11/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mdropout(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minplace)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/3350 /lib/python3.11/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout(\u001b[39minput\u001b[39m, p, training)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize lists to store information from each fold and prediction\n",
    "fold_results = []   # List to store information from each fold\n",
    "f1_score_list = []  # List to store F1 scores for each fold\n",
    "accuracy_list = []  # List to store accuracy scores for each fold\n",
    "\n",
    "# Initialize misclassifications dictionary\n",
    "party_classifications_dict = defaultdict(int)\n",
    "\n",
    "# Iterate over folds using StratifiedKFold\n",
    "for i, (train_index, test_index) in enumerate(skf.split(merged_outlets['Article_Content'], merged_outlets['party_label'])):\n",
    "    print(f\"Fold {i}:\")\n",
    "    \n",
    "    # Split the data into train and test sets for the current fold\n",
    "    train_data_fold = merged_outlets.iloc[train_index]\n",
    "    test_data_fold = merged_outlets.iloc[test_index]\n",
    "\n",
    "    train_texts_fold = train_data_fold['Article_Content'].tolist()\n",
    "    train_labels_fold = train_data_fold['party_label'].tolist()\n",
    "\n",
    "    test_texts_fold = test_data_fold['Article_Content'].tolist()\n",
    "    test_labels_fold = test_data_fold['party_label'].tolist()\n",
    "\n",
    "    print('Lengths of train and tests texts and labels: ') \n",
    "    print('Train texts: ', len(train_texts_fold))\n",
    "    print('Train labels: ', len(train_labels_fold))\n",
    "    print('Test texts: ', len(test_texts_fold))\n",
    "    print('Test labels: ', len(test_labels_fold))\n",
    "\n",
    "    # Define label2id and id2label based on unique labels in the current fold\n",
    "    unique_labels = set(train_labels_fold + test_labels_fold)\n",
    "    label2id = {label: id for id, label in enumerate(unique_labels)}\n",
    "    id2label = {id: label for label, id in label2id.items()}\n",
    "\n",
    "    # Tokenize the articles using DistilBertTokenizerFast for the current fold\n",
    "    train_encodings_fold = tokenizer(list(train_texts_fold), truncation=True, padding=True, max_length=512)\n",
    "    test_encodings_fold = tokenizer(list(test_texts_fold), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "    train_labels_encoded_fold = [label2id[y] for y in train_labels_fold]\n",
    "    test_labels_encoded_fold = [label2id[y] for y in test_labels_fold]\n",
    "\n",
    "    # Create datasets for the current fold\n",
    "    train_dataset_fold = MyDataset(train_encodings_fold, train_labels_encoded_fold)\n",
    "    test_dataset_fold = MyDataset(test_encodings_fold, test_labels_encoded_fold)\n",
    "\n",
    "    # Create and train the model for the current fold using DistilBertForSequenceClassification\n",
    "    model_fold = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=len(id2label)).to('cpu')\n",
    "\n",
    "    trainer_fold = Trainer(\n",
    "        model=model_fold,                    # instantiated Transformers model to be trained\n",
    "        args=training_args,                  # Training arguments initialized above\n",
    "        train_dataset=train_dataset_fold,    # Training dataset\n",
    "        eval_dataset=test_dataset_fold,      # Evaluation dataset\n",
    "        compute_metrics=compute_metrics      # Custom evaluation function\n",
    "    )\n",
    "\n",
    "    trainer_fold.train()\n",
    "    \n",
    "    trainer_fold.evaluate()\n",
    "\n",
    "    # Get predictions on the test set\n",
    "    predictions_fold = trainer_fold.predict(test_dataset_fold)\n",
    "\n",
    "    predicted_labels = predictions_fold.predictions.argmax(-1)  # Get the highest probability prediction\n",
    "    predicted_labels = predicted_labels.flatten().tolist()      # Flatten the predictions into a 1D list\n",
    "    predicted_labels = [id2label[l] for l in predicted_labels]  # Convert from integers back to strings for readability\n",
    "    predicted_labels_num = [label2id[label] for label in predicted_labels]\n",
    "\n",
    "    print('Length of Predicted Labels: ', len(predicted_labels))\n",
    "    print('Printed Predicted Labels: ', predicted_labels[:21])\n",
    "    print('\\nClassification Report: ')\n",
    "    print(classification_report(test_labels_encoded_fold, predicted_labels_num))\n",
    "\n",
    "    # Calculate and store F1 score and accuracy for the current fold\n",
    "    f1_score_list.append(f1_score(test_labels_encoded_fold, predicted_labels_num, average='weighted'))\n",
    "    accuracy_list.append(accuracy_score(test_labels_encoded_fold, predicted_labels_num))\n",
    "\n",
    "    # Extract information for the current fold and store it in the overall list\n",
    "    fold_info = [(id2label[true_label], id2label[predicted_label], text) for true_label, predicted_label, text in zip(test_labels_encoded_fold, predicted_labels_num, test_texts_fold)]\n",
    "    fold_results.append(fold_info)\n",
    "    \n",
    "    # If it's the last fold, update tag_classifications_dict\n",
    "    if i == (n_splits - 1):\n",
    "        for true_label, predicted_label, _ in fold_info:\n",
    "            if true_label != predicted_label:\n",
    "                party_classifications_dict[(true_label, predicted_label)] += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3350 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
