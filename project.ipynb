{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project guidelines\n",
    "\n",
    "**Note:** Use these guidelines if and only if you are pursuing a **final project of your own design**. For those taking the final exam instead of the project, see the (separate) final exam notebook.\n",
    "\n",
    "## Guidelines\n",
    "\n",
    "These guidelines are intended for **undergraduates enrolled in INFO 3350**. If you are a graduate student enrolled in INFO 6350, you're welcome to consult the information below, but you have wider latitude to design and develop your project in line with your research goals.\n",
    "\n",
    "### The task\n",
    "\n",
    "Your task is to: identify an interesting problem connected to the humanities or humanistic social sciences that's addressable with the help of computational methods, formulate a hypothesis about it, devise an experiment or experiments to test your hypothesis, present the results of your investigations, and discuss your findings.\n",
    "\n",
    "These tasks essentially replicate the process of writing an academic paper. You can think of your project as a paper in miniature.\n",
    "\n",
    "You are free to present each of these tasks as you see fit. You should use narrative text (that is, your own writing in a markdown cell), citations of others' work, numerical results, tables of data, and static and/or interactive visualizations as appropriate. Total length is flexible and depends on the number of people involved in the work, as well as the specific balance you strike between the ambition of your question and the sophistication of your methods. But be aware that numbers never, ever speak for themselves. Quantitative results presented without substantial discussion will not earn high marks. \n",
    "\n",
    "Your project should reflect, at minimum, ten **or more** hours of work by each participant, though you will be graded on the quality of your work, not the amount of time it took you to produce it. Most high-quality projects represent twenty or more hours of work by each member.\n",
    "\n",
    "#### Pick an important and interesting problem!\n",
    "\n",
    "No amount of technical sophistication will overcome a fundamentally uninteresting problem at the core of your work. You have seen many pieces of successful computational humanities research over the course of the semester. You might use these as a guide to the kinds of problems that interest scholars in a range of humanities disciplines. You may also want to spend some time in the library, reading recent books and articles in the professional literature. **Problem selection and motivation are integral parts of the project.** Do not neglect them.\n",
    "\n",
    "### Format\n",
    "\n",
    "You should submit your project as a Jupyter notebook, along with all data necessary to reproduce your analysis. If your dataset is too large to share easily, let us know in advance so that we can find a workaround. If you have a reason to prefer a presentation format other than a notebook, likewise let us know so that we can discuss the options.\n",
    "\n",
    "Your report should have four basic sections (provided in cells below for ease of reference):\n",
    "\n",
    "1. **Introduction and hypothesis.** What problem are you working on? Why is it interesting and important? What have other people said about it? What do you expect to find?\n",
    "2. **Corpus, data, and methods.** What data have you used? Where did it come from? How did you collect it? What are its limitations or omissions? What major methods will you use to analyze it? Why are those methods the appropriate ones?\n",
    "3. **Results.** What did you find? How did you find it? How should we read your figures? Be sure to include confidence intervals or other measures of statistical significance or uncetainty where appropriate.\n",
    "4. **Discussion and conclusions.** What does it all mean? Do your results support your hypothesis? Why or why not? What are the limitations of your study and how might those limitations be addressed in future work?\n",
    "\n",
    "Within each of those sections, you may use as many code and markdown cells as you like. You may, of course, address additional questions or issues not listed above.\n",
    "\n",
    "All code used in the project should be present in the notebook (except for widely-available libraries that you import), but **be sure that we can read and understand your report in full without rerunning the code**. Be sure, too, to explain what you're doing along the way, both by describing your data and methods and by writing clean, well commented code.\n",
    "\n",
    "### Grading\n",
    "\n",
    "This project takes the place of the take-home final exam for the course. It is worth 35% of your overall grade. You will be graded on the quality and ambition of each aspect of the project. No single component is more important than the others.\n",
    "\n",
    "### Practical details\n",
    "\n",
    "* The project is due at **noon on Saturday, December 9** via upload to CMS of a single zip file containing your fully executed Jupyter notebook and all associated data.\n",
    "* You may work alone or in a group of up to three total members.\n",
    "    * If you work in a group, be sure to list the names of the group members.\n",
    "    * For groups, create your group on CMS and submit one notebook for the entire group. **Each group should also submit a statement of responsibility** that describes in general terms who performed which parts of the project.\n",
    "* You may post questions on Ed, but should do so privately (visible to course staff only).\n",
    "* Interactive visualizations do not always work when embedded in shared notebooks. If you plan to use interactives, you may need to host them elsewhere and link to them.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your info\n",
    "* NetID(s): sc2548, jsc342\n",
    "* Name(s): Stephy Chen, Joyce Chen\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brainstorm Ideas for Project ##\n",
    "\n",
    "1. Examining lyrics of songs from varying genres, artists, and etc. (possibly taken from billboard, top 100 charts, etc) to see whether they reflect popular trends during that period of time either based on the current events occuring (may include poltically or socially motivated events)  --\n",
    "\n",
    "2. Examining how authors (perhaps classical authors from a certain range of time/genre) used adjectives/certain connotative words/phrases to describe gender? (Could we predict information about the author through these used phrases)\n",
    "\n",
    "3. Examining song lyrics from varying genres/artists to determine how they describe gender. Seeing whether the language used or connotation of their words reflect a trend in the type of artists that sang or wrote the lyrics.\n",
    "\n",
    "4. Examining media and news outlets diction in describing current events relating to political partisanship (whether their words indicate the general party they're leaning towards)\n",
    "   - Taking at whether the words used to describe certain events are more comparably positive/negative in connotation\n",
    "   - Seeing whether there is a distinction between the type of words used to describe current events by left/right wing\n",
    "   - implications: \n",
    "     - Being aware of echo chambers \n",
    "     - Recommending news outlets/media that are more neutral based\n",
    "     - Indicating which news outlets/media are more biased towards a side\n",
    "     - Is there a correlation with the recent political polarization \n",
    "https://www.mediacloud.org/media-cloud-directory \n",
    "\n",
    "5. Examine reddit and discussion forums to understand incel culture/crimes against women descriptions to see \n",
    "\n",
    "\n",
    "FINAL DECISION: CHOICE 4 \n",
    "\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In an era marked by unprecedented political polarization, the focus on media and its role in shaping public perception has never been more critical. This data project delves into the intricate web of language, word choice, and stylistic choices employed by various news outlets when reporting on current events, with a keen emphasis on political partisanship.\n",
    "\n",
    "The importance of this investigation lies in its potential to uncover implicit biases within media narratives and disseminated information. Firstly, we scrutinize the diction and connotations used to describe events among news outlets, seeking to discern patterns that determine whether their choice of words or coverage of events indicates a leaning towards a particular political party. Simultaneously, we conduct a comparative analysis of language used by different media outlets (across the political spectrum) when covering the same events, unraveling distinct narratives crafted by left-wing and right-wing sources to evaluate the implications of these differing perspectives. Secondly, the exploration extends beyond mere observation, delving into whether certain events are portrayed with a comparably positive or negative bias, contributing to the broader discourse on media objectivity.\n",
    "\n",
    "The significance of this research is far-reaching and transcends academic curiosity. It holds profound implications for societal awareness and media literacy. By making citizens aware of potential echo chambers in media consumption, the project aims to empower individuals to critically evaluate the information they receive. Beyond mere awareness, our project aspires to offer recommendations for news outlets and media that demonstrate a more neutral stance, allowing consumers to make informed choices about their news sources.\n",
    "\n",
    "Existing literature reviews have already shed light on the political slant present in many media sources (the picture below from AllSides is widely used when looking at the media outlets). This project builds upon this foundation, seeking not only to confirm these biases but also to provide a nuanced understanding of the language that perpetuates them. As political polarization continues to shape the socio-political landscape, this research contributes to the ongoing dialogue by exploring the correlation between media language and the evolving dynamics of political partisanship.\n",
    "\n",
    "![image](pictures/all-sides.jpeg)\n",
    "\n",
    "https://www.allsides.com/media-bias/media-bias-chart \n",
    "\n",
    "https://www.pewresearch.org/journalism/2014/10/21/section-1-media-sources-distinct-favorites-emerge-on-the-left-and-right/\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Research Question and Hypothesis\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "We hypothesize that there is a correlation between the language and style employed by news outlets in reporting current events and political partisanship. \n",
    "\n",
    "(if specificity is needed, here is a version: We hypothesize that there is a positive correlation between the language and style employed by news outlets in reporting current events and political partisanship. Specifically, we anticipate that news outlets leaning towards a particular political party will use language and style that align with the ideologies of that party.)\n",
    "\n",
    "If would be better for us to focus on a specific topic. \n",
    "Single issue (couple of hundred of articles) about two outlets. \n",
    "\n",
    "### Research Question \n",
    "\n",
    "Do news outlets' language and style in reporting current events correlate with political partisanship? How much do these language choices link to the perceived positivity or negativity of specific events?\n",
    "\n",
    "### Selected News Outlets \n",
    "\n",
    "Out of all these outlets seen in the AllSides graph, we will choose 3 from each category to analyze the articles published. (Should we have a separate section where we specifically pick articles from different sources that are convering the same events or is that included within the 3 that we choose from each category?)\n",
    "\n",
    "#### Left \n",
    "- CNN \n",
    "- Buzzfeed News \n",
    "- HuffPost \n",
    "- MSNBC \n",
    "- The New Yorker \n",
    "\n",
    "#### Left-Leaning \n",
    "- Bloomberg \n",
    "- CBS \n",
    "- NBC \n",
    "- New York Times News\n",
    "- Washington Post \n",
    "- USA Today \n",
    "\n",
    "#### Center\n",
    "- Wall Street Journal News \n",
    "- Reuters \n",
    "- Newsweek \n",
    "- BBC \n",
    "- Reuters \n",
    "- The Hill \n",
    "\n",
    "#### Right-Leaning \n",
    "- Epoch Times \n",
    "- The Washington Times \n",
    "- The Post Millennial \n",
    "- The American Conservative \n",
    "- The Dispatch \n",
    "\n",
    "#### Right\n",
    "- Daily Mail \n",
    "- Daily Wire \n",
    "- Fox News \n",
    "- The Federalist \n",
    "- The American Spectator\n",
    "\n",
    "\n",
    "\n",
    "OFFICIAL LIST\n",
    "\n",
    "#### Left \n",
    "- CNN \n",
    "\n",
    "#### Left-Leaning \n",
    "- New York Times News\n",
    "\n",
    "#### Center\n",
    "- Wall Street Journal News (Optional)\n",
    "- BBC \n",
    "\n",
    "#### Right-Leaning \n",
    "- The New York Post News\n",
    "\n",
    "#### Right \n",
    "- Fox News \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Corpus & Data Cleaning\n",
    "\n",
    "### Corpus Creation: DEADLINE (12/05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Scrape a ridiculous amount of articles from news outlets\n",
    "\n",
    "2. Read relevant papers (on abortion and the work that has been done to analyze thus far) \n",
    "http://languagelog.ldc.upenn.edu/myl/Monroe.pdf\n",
    "https://www.pewresearch.org/religion/fact-sheet/public-opinion-on-abortion/#CHAPTER-h-views-on-abortion-2021-a-detailed-look \n",
    "\n",
    "3. Standard of Comparison Creation Part I: Examine the partisan of congressional speeches to create the spectrum of words that indicate whether certain phrasing belongs to a certain party\n",
    "https://data.stanford.edu/congress_text (scroll down on page to find multiple zip files)\n",
    "\n",
    "4. Standard of Comparison Creation Part II: Examine presidential debates regarding abortion and use that as a standard to characterize the political stance on abortion (whether they use or have positive/negative views) \n",
    "https://www.debates.org/voter-education/debate-transcripts/   \n",
    "\n",
    "- Finding similarity of the phrasings used between the standards created (backed up by scholarly articles) with the phrasings commonly found within news articles\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping News Outlet Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Articles (51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News_Outlet</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Publication_Date</th>\n",
       "      <th>Article_Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Abortion is ancient history: Long before Roe, ...</td>\n",
       "      <td>Katie Hunt</td>\n",
       "      <td>Published 7:29 AM EDT, Fri June 23, 2023</td>\n",
       "      <td>CNN  — Abortion today, at least in the United ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Another state passed a near-total abortion ban...</td>\n",
       "      <td>Zachary B. Wolf</td>\n",
       "      <td>Updated 2:13 AM EDT, Wed September 14, 2022</td>\n",
       "      <td>CNN  — Good luck trying to keep on top of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Myths about abortion and women’s mental health...</td>\n",
       "      <td>Sandee LaMotte</td>\n",
       "      <td>Updated 11:12 PM EDT, Mon July 11, 2022</td>\n",
       "      <td>CNN  — It’s an unfounded message experts say i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Survey finds widespread confusion around medic...</td>\n",
       "      <td>Deidre McPhillips</td>\n",
       "      <td>Published 6:13 AM EST, Wed February 1, 2023</td>\n",
       "      <td>CNN  — Nearly half of adults in the United Sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN</td>\n",
       "      <td>Births have increased in states with abortion ...</td>\n",
       "      <td>Deidre McPhillips</td>\n",
       "      <td>Published 1:22 PM EST, Tue November 21, 2023</td>\n",
       "      <td>CNN  —  Nearly a quarter of people seeking an ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  News_Outlet                                              Title  \\\n",
       "0         CNN  Abortion is ancient history: Long before Roe, ...   \n",
       "1         CNN  Another state passed a near-total abortion ban...   \n",
       "2         CNN  Myths about abortion and women’s mental health...   \n",
       "3         CNN  Survey finds widespread confusion around medic...   \n",
       "4         CNN  Births have increased in states with abortion ...   \n",
       "\n",
       "              Author                              Publication_Date  \\\n",
       "0         Katie Hunt      Published 7:29 AM EDT, Fri June 23, 2023   \n",
       "1    Zachary B. Wolf   Updated 2:13 AM EDT, Wed September 14, 2022   \n",
       "2     Sandee LaMotte       Updated 11:12 PM EDT, Mon July 11, 2022   \n",
       "3  Deidre McPhillips   Published 6:13 AM EST, Wed February 1, 2023   \n",
       "4  Deidre McPhillips  Published 1:22 PM EST, Tue November 21, 2023   \n",
       "\n",
       "                                     Article_Content  \n",
       "0  CNN  — Abortion today, at least in the United ...  \n",
       "1  CNN  — Good luck trying to keep on top of the ...  \n",
       "2  CNN  — It’s an unfounded message experts say i...  \n",
       "3  CNN  — Nearly half of adults in the United Sta...  \n",
       "4  CNN  —  Nearly a quarter of people seeking an ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_txt_to_dataframe(file_path):\n",
    "    # Initialize empty lists to store data\n",
    "    news_outlets = []\n",
    "    titles = []\n",
    "    authors = []\n",
    "    publication_dates = []\n",
    "    article_contents = []\n",
    "\n",
    "    # Open the text file\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        # Initialize variables to store information\n",
    "        current_article = {}\n",
    "        \n",
    "        # Initialize list to store dictionaries\n",
    "        articles_data = []\n",
    "        \n",
    "        # Iterate through lines in the file\n",
    "        for line in lines:\n",
    "            # Split the line into key and value if possible\n",
    "            line_parts = line.split(':', 1)\n",
    "            \n",
    "            # Check if the line can be split into key and value\n",
    "            if len(line_parts) == 2:\n",
    "                key, value = map(str.strip, line_parts)\n",
    "                \n",
    "                # Check for the end of an article\n",
    "                if key == 'Article_Content':\n",
    "                    # Save the current article information\n",
    "                    current_article['Article_Content'] = value.strip()\n",
    "                    \n",
    "                    articles_data.append({\n",
    "                        'News_Outlet': current_article.get('News_Outlet', ''),\n",
    "                        'Title': current_article.get('Title', ''),\n",
    "                        'Author': current_article.get('Author', ''),\n",
    "                        'Publication_Date': current_article.get('Publication_Date', ''),\n",
    "                        'Article_Content': current_article.get('Article_Content', '')\n",
    "                    })\n",
    "\n",
    "                    current_article = {}\n",
    "                else:\n",
    "                    # Add key-value pair to current article dictionary\n",
    "                    current_article[key] = value\n",
    "\n",
    "    df = pd.DataFrame(articles_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "txt_file_path = 'CNN.txt'\n",
    "\n",
    "cnn_data_frame = read_txt_to_dataframe(txt_file_path)\n",
    "\n",
    "cnn_data_frame.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size is 6043\n",
      "Comparing language...\n",
      "Top terms\n",
      "abortion\t0.53810458372724\n",
      "said\t0.4567316288827305\n",
      "states\t0.40137003019003803\n",
      "women\t0.4010128040413933\n",
      "state\t0.3912555396715251\n",
      "people\t0.37129953244701064\n",
      "court\t0.36738965079280955\n",
      "health\t0.36332280401687467\n",
      "pregnancy\t0.3606955093391477\n",
      "law\t0.35634462719211507\n",
      "\n",
      "\n",
      "Top terms\n",
      "joke\t-0.16228342705665835\n",
      "logistical\t-0.16228342705665835\n",
      "longterm\t-0.16228342705665835\n",
      "longtime\t-0.16228342705665835\n",
      "lookout\t-0.16228342705665835\n",
      "loom\t-0.16228342705665835\n",
      "looming\t-0.16228342705665835\n",
      "loosening\t-0.16228342705665835\n",
      "lord\t-0.16228342705665835\n",
      "losses\t-0.16228342705665835\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import fightinwords as fw\n",
    "\n",
    "# Extract 'Article_Content' column \n",
    "article_contents = cnn_data_frame['Article_Content'].values\n",
    "\n",
    "# Vectorize using CountVectorizer\n",
    "vec = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    strip_accents='unicode',\n",
    "    input='content',\n",
    "    encoding='utf-8',\n",
    "    stop_words='english'\n",
    ")\n",
    "X = vec.fit_transform(article_contents)\n",
    "\n",
    "# Calculate Fighting Words values\n",
    "fighting_words_results = fw.bayes_compare_language(\n",
    "    l1=np.where(np.ones(len(article_contents)))[0],  # indices for all articles\n",
    "    l2=[],  # empty list since we're comparing language within the same set\n",
    "    features=X,\n",
    "    ngram=1,\n",
    "    prior=0.01,\n",
    "    prior_weight=None,\n",
    "    cv=vec,\n",
    "    vocab=list(vec.get_feature_names_out())\n",
    ")\n",
    "\n",
    "# Output the results\n",
    "def fw_score(data, n=10):\n",
    "    def print_top_terms(data):\n",
    "        print(f'Top terms')\n",
    "        for term, score in data:\n",
    "            print(f'{term}\\t{score}')\n",
    "\n",
    "    print_top_terms(reversed(data[-n:]))\n",
    "    print('\\n')\n",
    "    print_top_terms(data[:n])\n",
    "\n",
    "fw_score(fighting_words_results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping Presidential Elections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scrape the raw text from presidential elections that mention abortions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/stephy/opt/anaconda3/envs/info2950/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/stephy/opt/anaconda3/envs/info2950/lib/python3.10/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/stephy/opt/anaconda3/envs/info2950/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/stephy/opt/anaconda3/envs/info2950/lib/python3.10/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/stephy/opt/anaconda3/envs/info2950/lib/python3.10/site-packages (from requests) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "##install requests\n",
    "\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /Users/stephy/opt/anaconda3/envs/info2950/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/stephy/opt/anaconda3/envs/info2950/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/stephy/opt/anaconda3/envs/info2950/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Users/stephy/opt/anaconda3/envs/info2950/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     name  year dem/rep left/right  \\\n",
      "0   PENCE  2020     rep      right   \n",
      "1   PENCE  2020     rep      right   \n",
      "2   PENCE  2020     rep      right   \n",
      "3  HARRIS  2020     dem       left   \n",
      "4   PENCE  2020     rep      right   \n",
      "\n",
      "                                             content  \n",
      "0   Well thank you for the question, but I’ll use...  \n",
      "1   My hope is that when the hearing takes place,...  \n",
      "2   – treated respectfully and voted and confirme...  \n",
      "3   Thank you, Susan. First of all, Joe Biden and...  \n",
      "4   Well, thank you, Susan. Let me just say, addr...  \n",
      "(5, 5)\n"
     ]
    }
   ],
   "source": [
    "#Scraped Data for Vice Presidential Election 2020 (Kamala Harris and Mike Pence)\n",
    "\n",
    "URL = \"https://debates.org/voter-education/debate-transcripts/vice-presidential-debate-at-the-university-of-utah-in-salt-lake-city-utah/\" \n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "all_paragraphs = soup.find_all('p')\n",
    "\n",
    "# Find the indices of paragraphs containing the word \"abortion\"\n",
    "abortion_indices = [index for index, p in enumerate(all_paragraphs) if 'abortion' in p.get_text().lower()]\n",
    "\n",
    "if abortion_indices:\n",
    "    # Extract paragraphs between the first and last mention of \"abortion\"\n",
    "    start_index = abortion_indices[0]\n",
    "    end_index = abortion_indices[-1]\n",
    "    \n",
    "    paragraphs_between_abortion = all_paragraphs[start_index:end_index + 1]\n",
    "\n",
    "    # Create a DataFrame with columns for 'name', 'dem/rep', and 'content'\n",
    "    data = {'name': [], 'year': [], 'dem/rep': [], 'left/right': [], 'content': []}\n",
    "    count_pence = 0\n",
    "\n",
    "    for paragraph in paragraphs_between_abortion:\n",
    "        count_pence += paragraph.get_text().count('PENCE:')\n",
    "        \n",
    "        if count_pence == 4:\n",
    "            # Modify the paragraph in-place if it's the fourth instance of 'PENCE:'\n",
    "            paragraph.string = re.sub(r'^PENCE:', 'HARRIS:', paragraph.get_text())\n",
    "    \n",
    "        # Skip paragraphs that start with \"PAGE:\"\n",
    "        if not paragraph.get_text().startswith(\"PAGE:\"):\n",
    "            # Determine 'name' and 'dem/rep' based on the speaker\n",
    "            if 'PENCE:' in paragraph.get_text():\n",
    "                data['name'].append('PENCE')\n",
    "                data['year'].append(2020)\n",
    "                data['dem/rep'].append('rep')\n",
    "                data['left/right'].append('right')\n",
    "            else:\n",
    "                data['name'].append('HARRIS')\n",
    "                data['year'].append(2020)\n",
    "                data['dem/rep'].append('dem')\n",
    "                data['left/right'].append('left')\n",
    "            \n",
    "            data['content'].append(re.sub(r'^PENCE:|HARRIS:', '', paragraph.get_text()))\n",
    "\n",
    "    prez_df_2020 = pd.DataFrame(data)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(prez_df_2020)\n",
    "    print(prez_df_2020.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     name  year dem/rep left/right  \\\n",
      "0   PENCE  2016     rep      right   \n",
      "1   PENCE  2016     rep      right   \n",
      "2   PENCE  2016     rep      right   \n",
      "3   PENCE  2016     rep      right   \n",
      "4   PENCE  2016     rep      right   \n",
      "5   KAINE  2016     dem       left   \n",
      "6   KAINE  2016     dem       left   \n",
      "7   KAINE  2016     dem       left   \n",
      "8   KAINE  2016     dem       left   \n",
      "9   KAINE  2016     dem       left   \n",
      "10  KAINE  2016     dem       left   \n",
      "11  PENCE  2016     rep      right   \n",
      "12  KAINE  2016     dem       left   \n",
      "13  PENCE  2016     rep      right   \n",
      "14  KAINE  2016     dem       left   \n",
      "15  PENCE  2016     rep      right   \n",
      "16  KAINE  2016     dem       left   \n",
      "17  PENCE  2016     rep      right   \n",
      "18  KAINE  2016     dem       left   \n",
      "19  PENCE  2016     rep      right   \n",
      "20  KAINE  2016     dem       left   \n",
      "21  PENCE  2016     rep      right   \n",
      "22  KAINE  2016     dem       left   \n",
      "23  PENCE  2016     rep      right   \n",
      "24  KAINE  2016     dem       left   \n",
      "25  PENCE  2016     rep      right   \n",
      "26  KAINE  2016     dem       left   \n",
      "27  PENCE  2016     rep      right   \n",
      "28  KAINE  2016     dem       left   \n",
      "29  PENCE  2016     rep      right   \n",
      "30  PENCE  2016     rep      right   \n",
      "31  PENCE  2016     rep      right   \n",
      "\n",
      "                                              content  \n",
      "0    But for me, I would tell you that for me the ...  \n",
      "1   The state of Indiana has also sought to make s...  \n",
      "2   But what I can’t understand is with Hillary Cl...  \n",
      "3   And I cannot — I can’t conscience about — abou...  \n",
      "4   So for me, my faith informs my life. I try and...  \n",
      "5    Elaine, this is a fundamental question, a fun...  \n",
      "6   But we really feel like you should live fully ...  \n",
      "7   So let’s talk about abortion and choice. Let’s...  \n",
      "8   And we don’t think that women should be punish...  \n",
      "9   Governor Pence wants to repeal Roe v. Wade. He...  \n",
      "10  I think you should live your moral values. But...  \n",
      "11   No, it’s really not. Donald Trump and I would...  \n",
      "12                Then why did Donald Trump say that?  \n",
      "13                               We just never would.  \n",
      "14                               Why did he say that?  \n",
      "15   Well, look, it’s — look, he’s not a polished ...  \n",
      "16   Well, I would admit that’s not a polished…[cr...  \n",
      "17   You know, things don’t always come out exactl...  \n",
      "18                                   Well, can I say…  \n",
      "19   But I’m telling you what the policy of our ad...  \n",
      "20   Great line from the — great line from the gos...  \n",
      "21                                              Yeah.  \n",
      "22   When Donald Trump says women should be punish...  \n",
      "23                                   I’m telling you…  \n",
      "24   … or John McCain is not a hero, he is showing...  \n",
      "25   Senator, you’ve whipped out that Mexican thin...  \n",
      "26                                 Can you defend it?  \n",
      "27   There are criminal aliens in this country, Ti...  \n",
      "28   You want to — you want to use a big broad bru...  \n",
      "29   He also said and many of them are good people...  \n",
      "30  But here’s — there is a choice, and it is a ch...  \n",
      "31  And I have appreciated the fact that you’ve su...  \n",
      "(32, 5)\n"
     ]
    }
   ],
   "source": [
    "#Scraped Data for Vice Presidential Election 2016 (Mike Pence and Tim Kaine)\n",
    "\n",
    "#October 4, 2016\n",
    "\n",
    "URL = \"https://www.debates.org/voter-education/debate-transcripts/october-4-2016-debate-transcript/\"\n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "all_paragraphs = soup.find_all('p')\n",
    "\n",
    "# Find the index of the paragraph with the last mention of \"abortion\"\n",
    "last_abortion_index = max([index for index, p in enumerate(all_paragraphs) if 'abortion' in p.get_text().lower()], default=0)\n",
    "\n",
    "# Find the index of the paragraph starting with \"PENCE: But for me\"\n",
    "pence_index = next((i for i, p in enumerate(all_paragraphs) if p.get_text().startswith(\"PENCE: But for me\")), None)\n",
    "\n",
    "# Extract paragraphs between \"PENCE: But for me\" and the last mention of \"abortion\"\n",
    "paragraphs_selected = all_paragraphs[pence_index:last_abortion_index + 1]\n",
    "\n",
    "# Create a DataFrame with columns for 'name', 'dem/rep', 'left/right', and 'content'\n",
    "data = {'name': [], 'year': [], 'dem/rep': [], 'left/right': [], 'content': []}\n",
    "\n",
    "current_speaker = None\n",
    "\n",
    "for paragraph in paragraphs_selected:\n",
    "    text = paragraph.get_text()\n",
    "    \n",
    "    # Skip paragraphs that start with \"QUIJANO:\"\n",
    "    if text.startswith(\"QUIJANO:\"):\n",
    "        continue\n",
    "    \n",
    "    # Determine 'name' and 'dem/rep' based on the speaker\n",
    "    if 'PENCE:' in text:\n",
    "        current_speaker = 'PENCE'\n",
    "        dem_rep = 'rep'\n",
    "    elif 'KAINE:' in text:\n",
    "        current_speaker = 'KAINE'\n",
    "        dem_rep = 'dem'\n",
    "    \n",
    "    # Determine 'left/right' based on the speaker\n",
    "    if 'PENCE:' in text:\n",
    "        left_right = 'right'\n",
    "    elif 'KAINE:' in text:\n",
    "        left_right = 'left'\n",
    "    \n",
    "    # Append data to the dictionary\n",
    "    data['name'].append(current_speaker)\n",
    "    data['year'].append(2016)\n",
    "    data['dem/rep'].append(dem_rep)\n",
    "    data['left/right'].append(left_right)\n",
    "    data['content'].append(re.sub(r'^PENCE:|KAINE:', '', paragraph.get_text()))\n",
    "    \n",
    "# Create a DataFrame from the dictionary\n",
    "prez_df_2016 = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(prez_df_2016)\n",
    "print(prez_df_2016.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       name  year dem/rep left/right  \\\n",
      "0     TRUMP  2016     rep      right   \n",
      "1     TRUMP  2016     rep      right   \n",
      "2     TRUMP  2016     rep      right   \n",
      "3     TRUMP  2016     rep      right   \n",
      "4   CLINTON  2016     dem       left   \n",
      "5   CLINTON  2016     dem       left   \n",
      "6   CLINTON  2016     dem       left   \n",
      "7   CLINTON  2016     dem       left   \n",
      "8   CLINTON  2016     dem       left   \n",
      "9   CLINTON  2016     dem       left   \n",
      "10    TRUMP  2016     rep      right   \n",
      "11    TRUMP  2016     rep      right   \n",
      "12  CLINTON  2016     dem       left   \n",
      "13  CLINTON  2016     dem       left   \n",
      "\n",
      "                                              content  \n",
      "0                                              Right.  \n",
      "1    Well, if that would happen, because I am pro-...  \n",
      "2    If they overturned it, it will go back to the...  \n",
      "3    Well, if we put another two or perhaps three ...  \n",
      "4    Well, I strongly support Roe v. Wade, which g...  \n",
      "5   So many states are putting very stringent regu...  \n",
      "6   Donald has said he’s in favor of defunding Pla...  \n",
      "7    And we have come too far to have that turned ...  \n",
      "8    Because Roe v. Wade very clearly sets out tha...  \n",
      "9   The kinds of cases that fall at the end of pre...  \n",
      "10   Well, I think it’s terrible. If you go with w...  \n",
      "11  Now, you can say that that’s OK and Hillary ca...  \n",
      "12   Well, that is not what happens in these cases...  \n",
      "13  You know, I’ve had the great honor of travelin...  \n",
      "(14, 5)\n"
     ]
    }
   ],
   "source": [
    "#Scraped Data for Third residential Election 2016 (Hilary Clinton and Donald Trump) \n",
    "\n",
    "#October 19, 2016\n",
    "\n",
    "URL = \"https://www.debates.org/voter-education/debate-transcripts/october-19-2016-debate-transcript/\" \n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "all_paragraphs = soup.find_all('p')\n",
    "\n",
    "# Find the indices of paragraphs containing the word \"abortion\"\n",
    "abortion_indices = [index for index, p in enumerate(all_paragraphs) if 'abortion' in p.get_text().lower()]\n",
    "\n",
    "if abortion_indices:\n",
    "    # Extract paragraphs between the first and last mention of \"abortion\"\n",
    "    start_index = abortion_indices[0]\n",
    "    end_index = abortion_indices[-1]\n",
    "    \n",
    "    paragraphs_between_abortion = all_paragraphs[start_index:end_index + 1]\n",
    "\n",
    "    # Create a DataFrame with columns for 'name', 'year', 'dem/rep', 'left/right', 'content'\n",
    "    data = {'name': [], 'year': [], 'dem/rep': [], 'left/right': [], 'content': []}\n",
    "\n",
    "    current_speaker = None\n",
    "\n",
    "    for paragraph in paragraphs_between_abortion:\n",
    "        # Skip paragraphs that start with \"WALLACE:\"\n",
    "        if not paragraph.get_text().startswith(\"WALLACE:\"):\n",
    "            # Determine 'name' and 'dem/rep' based on the speaker\n",
    "            if 'TRUMP:' in paragraph.get_text():\n",
    "                current_speaker = 'TRUMP'\n",
    "            elif 'CLINTON:' in paragraph.get_text():\n",
    "                current_speaker = 'CLINTON'\n",
    "            \n",
    "            data['name'].append(current_speaker)\n",
    "            data['year'].append(2016)\n",
    "            data['dem/rep'].append('rep' if current_speaker == 'TRUMP' else 'dem')\n",
    "            data['left/right'].append('right' if current_speaker == 'TRUMP' else 'left')\n",
    "            data['content'].append(re.sub(r'^TRUMP:|CLINTON:', '', paragraph.get_text()))\n",
    "\n",
    "    prez_df_2016_2 = pd.DataFrame(data)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(prez_df_2016_2)\n",
    "    print(prez_df_2016_2.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     name  year dem/rep left/right  \\\n",
      "0    RYAN  2012     rep      right   \n",
      "1    RYAN  2012     rep      right   \n",
      "2    RYAN  2012     rep      right   \n",
      "3    RYAN  2012     rep      right   \n",
      "4    RYAN  2012     rep      right   \n",
      "5   BIDEN  2012     dem       left   \n",
      "6   BIDEN  2012     dem       left   \n",
      "7   BIDEN  2012     dem       left   \n",
      "8    RYAN  2012     rep      right   \n",
      "9    RYAN  2012     rep      right   \n",
      "10  BIDEN  2012     dem       left   \n",
      "11  BIDEN  2012     dem       left   \n",
      "12   RYAN  2012     rep      right   \n",
      "13   RYAN  2012     rep      right   \n",
      "14  BIDEN  2012     dem       left   \n",
      "\n",
      "                                              content  \n",
      "0    I don’t see how a person can separate their p...  \n",
      "1    Now, you want to ask basically why I’m pro-li...  \n",
      "2   You know, I think about 10 1/2 years ago, my w...  \n",
      "3   That’s why — those are the reasons why I’m pro...  \n",
      "4   Our church should not have to sue our federal ...  \n",
      "5    My religion defines who I am, and I’ve been a...  \n",
      "6   But I refuse to impose it on equally devout Ch...  \n",
      "7   That is a fact. Now with regard to the way in ...  \n",
      "8    All I’m saying is, if you believe that life b...  \n",
      "9   Now, I’ve got to take issue with the Catholic ...  \n",
      "10                             You have on the issue…  \n",
      "11                                        (CROSSTALK)  \n",
      "12   … why would they keep — why would they keep s...  \n",
      "13   We don’t think that unelected judges should m...  \n",
      "14   The court — the next president will get one o...  \n",
      "(15, 5)\n"
     ]
    }
   ],
   "source": [
    "#Scraped Data for Vice Presidential Election 2012 (Biden and Ryan)\n",
    "\n",
    "#October 11, 2012\n",
    "\n",
    "URL = \"https://www.debates.org/voter-education/debate-transcripts/october-11-2012-the-biden-romney-vice-presidential-debate/\"\n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "all_paragraphs = soup.find_all('p')\n",
    "\n",
    "# Find the index of the paragraph containing \"RYAN: I don’t see how a\"\n",
    "ryan_paragraph_index = next((index for index, p in enumerate(all_paragraphs) if 'RYAN: I don’t see how a' in p.get_text()), None)\n",
    "\n",
    "# Find the indices of paragraphs containing the word \"abortion\" after the specified paragraph\n",
    "abortion_indices = [index for index, p in enumerate(all_paragraphs[ryan_paragraph_index:]) if 'abortion' in p.get_text().lower()]\n",
    "\n",
    "if abortion_indices:\n",
    "    # Adjust the start index to include the paragraph containing \"RYAN: I don’t see how a\"\n",
    "    start_index = ryan_paragraph_index\n",
    "    end_index = ryan_paragraph_index + abortion_indices[-1]\n",
    "    \n",
    "    paragraphs_between_abortion = all_paragraphs[start_index:end_index + 1]\n",
    "\n",
    "    # Create a DataFrame with columns for 'name', 'year', 'dem/rep', 'left/right', 'content'\n",
    "    data = {'name': [], 'year': [], 'dem/rep': [], 'left/right': [], 'content': []}\n",
    "\n",
    "    current_speaker = None\n",
    "\n",
    "    for paragraph in paragraphs_between_abortion:\n",
    "        # Skip paragraphs that start with \"RADDATZ:\"\n",
    "        if not paragraph.get_text().startswith(\"RADDATZ:\"):\n",
    "            # Determine 'name' and 'dem/rep' based on the speaker\n",
    "            if 'RYAN:' in paragraph.get_text():\n",
    "                current_speaker = 'RYAN'\n",
    "            elif 'BIDEN:' in paragraph.get_text():\n",
    "                current_speaker = 'BIDEN'\n",
    "            \n",
    "            data['name'].append(current_speaker)\n",
    "            data['year'].append(2012)\n",
    "            data['dem/rep'].append('rep' if current_speaker == 'RYAN' else 'dem')\n",
    "            data['left/right'].append('right' if current_speaker == 'RYAN' else 'left')\n",
    "            data['content'].append(re.sub(r'^RYAN:|BIDEN:', '', paragraph.get_text()))\n",
    "\n",
    "    prez_df_2012 = pd.DataFrame(data)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(prez_df_2012)\n",
    "    print(prez_df_2012.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      name  year dem/rep left/right  \\\n",
      "0   MCCAIN  2008     rep      right   \n",
      "1    OBAMA  2008     dem       left   \n",
      "2    OBAMA  2008     dem       left   \n",
      "3    OBAMA  2008     dem       left   \n",
      "4    OBAMA  2008     dem       left   \n",
      "5    OBAMA  2008     dem       left   \n",
      "6    OBAMA  2008     dem       left   \n",
      "7    OBAMA  2008     dem       left   \n",
      "8    OBAMA  2008     dem       left   \n",
      "9    OBAMA  2008     dem       left   \n",
      "10  MCCAIN  2008     rep      right   \n",
      "11  MCCAIN  2008     rep      right   \n",
      "12  MCCAIN  2008     rep      right   \n",
      "13  MCCAIN  2008     rep      right   \n",
      "14  MCCAIN  2008     rep      right   \n",
      "15  MCCAIN  2008     rep      right   \n",
      "16  MCCAIN  2008     rep      right   \n",
      "17   OBAMA  2008     dem       left   \n",
      "18   OBAMA  2008     dem       left   \n",
      "19   OBAMA  2008     dem       left   \n",
      "20   OBAMA  2008     dem       left   \n",
      "21   OBAMA  2008     dem       left   \n",
      "22   OBAMA  2008     dem       left   \n",
      "23   OBAMA  2008     dem       left   \n",
      "24   OBAMA  2008     dem       left   \n",
      "25   OBAMA  2008     dem       left   \n",
      "26  MCCAIN  2008     rep      right   \n",
      "27  MCCAIN  2008     rep      right   \n",
      "28  MCCAIN  2008     rep      right   \n",
      "\n",
      "                                              content  \n",
      "0    I would consider anyone in their qualificatio...  \n",
      "1    Well, I think it’s true that we shouldn’t app...  \n",
      "2   And it is true that this is going to be, I thi...  \n",
      "3   Now I would not provide a litmus test. But I a...  \n",
      "4   But what ultimately I believe is that women in...  \n",
      "5    So this is going to be an important issue. I ...  \n",
      "6   I’ll just give you one quick example. Senator ...  \n",
      "7   For years, she had been getting paid less than...  \n",
      "8   We tried to overturn it in the Senate. I suppo...  \n",
      "9   I think that it’s important for judges to unde...  \n",
      "10   Obviously, that law waived the statute of lim...  \n",
      "11  Let me talk to you about an important aspect o...  \n",
      "12  Senator Obama, as a member of the Illinois Sta...  \n",
      "13  And then, on the floor of the State Senate, as...  \n",
      "14  Then there was another bill before the Senate ...  \n",
      "15  I don’t know how you vote “present” on some of...  \n",
      "16  And he’ll say it has something to do with Roe ...  \n",
      "17   Yes, let me respond to this. If it sounds inc...  \n",
      "18  There was a bill that was put forward before t...  \n",
      "19  And the Illinois Medical Society, the organiza...  \n",
      "20  With respect to partial-birth abortion, I am c...  \n",
      "21  And I attempted, as many have in the past, of ...  \n",
      "22  The last point I want to make on the issue of ...  \n",
      "23  But there surely is some common ground when bo...  \n",
      "24  Those are all things that we put in the Democr...  \n",
      "25       We should try to reduce these circumstances.  \n",
      "26                                        Just again…  \n",
      "27   Just again, the example of the eloquence of S...  \n",
      "28  That’s the extreme pro-abortion position, quot...  \n",
      "(29, 5)\n"
     ]
    }
   ],
   "source": [
    "#Scraped Data for Presidential Election 2008 (Third McCain and Obama Debate)\n",
    "\n",
    "#October 15, 2008\n",
    "\n",
    "URL = \"https://www.debates.org/voter-education/debate-transcripts/october-15-2008-debate-transcript/\" \n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "all_paragraphs = soup.find_all('p')\n",
    "\n",
    "# Find the indices of paragraphs containing the word \"abortion\"\n",
    "abortion_indices = [index for index, p in enumerate(all_paragraphs) if 'abortion' in p.get_text().lower()]\n",
    "\n",
    "if abortion_indices:\n",
    "    # Extract paragraphs between the first and last mention of \"abortion\"\n",
    "    start_index = abortion_indices[0]\n",
    "    end_index = abortion_indices[-1]\n",
    "    \n",
    "    paragraphs_between_abortion = all_paragraphs[start_index:end_index + 1]\n",
    "\n",
    "    # Create a DataFrame with columns for 'name', 'year', 'dem/rep', 'left/right', 'content'\n",
    "    data = {'name': [], 'year': [], 'dem/rep': [], 'left/right': [], 'content': []}\n",
    "\n",
    "    current_speaker = None\n",
    "\n",
    "    for paragraph in paragraphs_between_abortion:\n",
    "        # Skip paragraphs that start with \"SCHIEFFER::\"\n",
    "        if not paragraph.get_text().startswith(\"SCHIEFFER:\"):\n",
    "            # Determine 'name' and 'dem/rep' based on the speaker\n",
    "            if 'MCCAIN:' in paragraph.get_text():\n",
    "                current_speaker = 'MCCAIN'\n",
    "            elif 'OBAMA:' in paragraph.get_text():\n",
    "                current_speaker = 'OBAMA'\n",
    "            \n",
    "            data['name'].append(current_speaker)\n",
    "            data['year'].append(2008)\n",
    "            data['dem/rep'].append('rep' if current_speaker == 'MCCAIN' else 'dem')\n",
    "            data['left/right'].append('right' if current_speaker == 'MCCAIN' else 'left')\n",
    "            data['content'].append(re.sub(r'^MCCAIN:|OBAMA:', '', paragraph.get_text()))\n",
    "\n",
    "    prez_df_2008 = pd.DataFrame(data)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(prez_df_2008)\n",
    "    print(prez_df_2008.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     name  year dem/rep left/right  \\\n",
      "0   KERRY  2004     dem       left   \n",
      "1   KERRY  2004     dem       left   \n",
      "2   KERRY  2004     dem       left   \n",
      "3   KERRY  2004     dem       left   \n",
      "4   KERRY  2004     dem       left   \n",
      "5   KERRY  2004     dem       left   \n",
      "6   KERRY  2004     dem       left   \n",
      "7   KERRY  2004     dem       left   \n",
      "8   KERRY  2004     dem       left   \n",
      "9   KERRY  2004     dem       left   \n",
      "10  KERRY  2004     dem       left   \n",
      "11  KERRY  2004     dem       left   \n",
      "12  KERRY  2004     dem       left   \n",
      "13   BUSH  2004     rep      right   \n",
      "14   BUSH  2004     rep      right   \n",
      "15   BUSH  2004     rep      right   \n",
      "16   BUSH  2004     rep      right   \n",
      "\n",
      "                                              content  \n",
      "0    I respect their views. I completely respect t...  \n",
      "1   I believe that I can’t legislate or transfer t...  \n",
      "2   I believe that choice is a woman’s choice. It’...  \n",
      "3   Now, I will not allow somebody to come in and ...  \n",
      "4   The president has never said whether or not he...  \n",
      "5   I will not. I will defend the right of Roe v. ...  \n",
      "6   Now, with respect to religion, you know, as I ...  \n",
      "7   And as President Kennedy said when he ran for ...  \n",
      "8   My faith affects everything that I do, in trut...  \n",
      "9   And I think that everything you do in public l...  \n",
      "10  That’s why I fight against poverty. That’s why...  \n",
      "11  That’s why I fight for equality and justice. A...  \n",
      "12  But I know this, that President Kennedy in his...  \n",
      "13   I think it’s important to promote a culture o...  \n",
      "14  Take, for example, the ban on partial birth ab...  \n",
      "15  What I’m saying is, is that as we promote life...  \n",
      "16  The last debate, my opponent said his wife was...  \n",
      "(17, 5)\n"
     ]
    }
   ],
   "source": [
    "#Scraped Data for Presidential Election 2004 (Bush and Kerry)\n",
    "\n",
    "#October 13, 2004\n",
    "\n",
    "URL = \"https://www.debates.org/voter-education/debate-transcripts/october-13-2004-debate-transcript/\" \n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "all_paragraphs = soup.find_all('p')\n",
    "\n",
    "# Find the index of the paragraph containing \"RYAN: I don’t see how a\"\n",
    "kerry_paragraph_index = next((index for index, p in enumerate(all_paragraphs) if 'KERRY: I respect their views.' in p.get_text()), None)\n",
    "\n",
    "# Find the indices of paragraphs containing the word \"abortion\" after the specified paragraph\n",
    "abortion_indices = [index for index, p in enumerate(all_paragraphs[kerry_paragraph_index:]) if 'abortion' in p.get_text().lower()]\n",
    "\n",
    "if abortion_indices:\n",
    "    # Adjust the start index to include the paragraph containing \"KERRY: I respect their views.\"\n",
    "    start_index = kerry_paragraph_index\n",
    "    end_index = kerry_paragraph_index + abortion_indices[-1]\n",
    "    \n",
    "    paragraphs_between_abortion = all_paragraphs[start_index:end_index + 1]\n",
    "\n",
    "    # Create a DataFrame with columns for 'name', 'year', 'dem/rep', 'left/right', 'content'\n",
    "    data = {'name': [], 'year': [], 'dem/rep': [], 'left/right': [], 'content': []}\n",
    "\n",
    "    current_speaker = None\n",
    "\n",
    "    for paragraph in paragraphs_between_abortion:\n",
    "        # Skip paragraphs that start with \"SCHIEFFER\"\n",
    "        if not paragraph.get_text().startswith(\"SCHIEFFER\"):\n",
    "            # Determine 'name' and 'dem/rep' based on the speaker\n",
    "            if 'BUSH:' in paragraph.get_text():\n",
    "                current_speaker = 'BUSH'\n",
    "            elif 'KERRY:' in paragraph.get_text():\n",
    "                current_speaker = 'KERRY'\n",
    "            \n",
    "            data['name'].append(current_speaker)\n",
    "            data['year'].append(2004)\n",
    "            data['dem/rep'].append('rep' if current_speaker == 'BUSH' else 'dem')\n",
    "            data['left/right'].append('right' if current_speaker == 'BUSH' else 'left')\n",
    "            data['content'].append(re.sub(r'^BUSH:|KERRY:', '', paragraph.get_text()))\n",
    "\n",
    "    prez_df_2004 = pd.DataFrame(data)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(prez_df_2004)\n",
    "    print(prez_df_2004.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     name  year dem/rep left/right  \\\n",
      "0   KERRY  2004     dem       left   \n",
      "1   KERRY  2004     dem       left   \n",
      "2   KERRY  2004     dem       left   \n",
      "3   KERRY  2004     dem       left   \n",
      "4   KERRY  2004     dem       left   \n",
      "5   KERRY  2004     dem       left   \n",
      "6   KERRY  2004     dem       left   \n",
      "7   KERRY  2004     dem       left   \n",
      "8   KERRY  2004     dem       left   \n",
      "9   KERRY  2004     dem       left   \n",
      "10   BUSH  2004     rep      right   \n",
      "11   BUSH  2004     rep      right   \n",
      "12   BUSH  2004     rep      right   \n",
      "13   BUSH  2004     rep      right   \n",
      "14   BUSH  2004     rep      right   \n",
      "15   BUSH  2004     rep      right   \n",
      "16   BUSH  2004     rep      right   \n",
      "17   BUSH  2004     rep      right   \n",
      "18   BUSH  2004     rep      right   \n",
      "19   BUSH  2004     rep      right   \n",
      "20   BUSH  2004     rep      right   \n",
      "21   BUSH  2004     rep      right   \n",
      "22  KERRY  2004     dem       left   \n",
      "23  KERRY  2004     dem       left   \n",
      "24  KERRY  2004     dem       left   \n",
      "25   BUSH  2004     rep      right   \n",
      "\n",
      "                                              content  \n",
      "0    I would say to that person exactly what I wil...  \n",
      "1   First of all, I cannot tell you how deeply I r...  \n",
      "2   But I can’t take what is an article of faith f...  \n",
      "3   But I can counsel people. I can talk reasonabl...  \n",
      "4   But as a president, I have to represent all th...  \n",
      "5   Now, I believe that you can take that position...  \n",
      "6   That’s why I think it’s important. That’s why ...  \n",
      "7                           You’ll help prevent AIDS.  \n",
      "8   You’ll help prevent unwanted children, unwante...  \n",
      "9   You’ll actually do a better job, I think, of p...  \n",
      "10                       I’m trying to decipher that.  \n",
      "11  My answer is, we’re not going to spend taxpaye...  \n",
      "12  This is an issue that divides America, but cer...  \n",
      "13  I signed the partial-birth — the ban on partia...  \n",
      "14  I think there ought to be parental notificatio...  \n",
      "15  I signed a bill called the Unborn Victims of V...  \n",
      "16  In other words, if you’re a mom and you’re pre...  \n",
      "17  These are reasonable ways to help promote a cu...  \n",
      "18  I also think we ought to continue to have good...  \n",
      "19  And we need to promote maternity group homes, ...  \n",
      "20  Culture of life is really important for a coun...  \n",
      "21                                         Thank you.  \n",
      "22   Well, again, the president just said, categor...  \n",
      "23  I’m against the partial-birth abortion, but yo...  \n",
      "24  Secondly, with respect to parental notificatio...  \n",
      "25   Well, it’s pretty simple when they say: Are y...  \n",
      "(26, 5)\n"
     ]
    }
   ],
   "source": [
    "#Scraped Data for Presidential Election 2004 (Bush and Kerry)\n",
    "\n",
    "#October 8, 2004\n",
    "\n",
    "URL = \"https://www.debates.org/voter-education/debate-transcripts/october-8-2004-debate-transcript/\" \n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "all_paragraphs = soup.find_all('p')\n",
    "\n",
    "# Find the index of the paragraph containing \"RYAN: I don’t see how a\"\n",
    "starting_index = next((index for index, p in enumerate(all_paragraphs) if 'DEGENHART: Senator Kerry, suppose' in p.get_text()), None)\n",
    "\n",
    "# Find the indices of paragraphs containing the word \"abortion\" after the specified paragraph\n",
    "abortion_indices = [index for index, p in enumerate(all_paragraphs[starting_index:]) if 'abortion' in p.get_text().lower()]\n",
    "\n",
    "if abortion_indices:\n",
    "    end_index = starting_index + abortion_indices[-1]\n",
    "    \n",
    "    paragraphs_between_abortion = all_paragraphs[starting_index:end_index + 1]\n",
    "\n",
    "    # Create a DataFrame with columns for 'name', 'year', 'dem/rep', 'left/right', 'content'\n",
    "    data = {'name': [], 'year': [], 'dem/rep': [], 'left/right': [], 'content': []}\n",
    "\n",
    "    current_speaker = None\n",
    "\n",
    "    for paragraph in paragraphs_between_abortion:\n",
    "        # Skip paragraphs that start with \"DEGENHART:\"\n",
    "        if not paragraph.get_text().startswith(\"DEGENHART:\") and not paragraph.get_text().startswith(\"GIBSON:\"):\n",
    "            # Determine 'name' and 'dem/rep' based on the speaker\n",
    "            if 'BUSH:' in paragraph.get_text():\n",
    "                current_speaker = 'BUSH'\n",
    "            elif 'KERRY:' in paragraph.get_text():\n",
    "                current_speaker = 'KERRY'\n",
    "            \n",
    "            data['name'].append(current_speaker)\n",
    "            data['year'].append(2004)\n",
    "            data['dem/rep'].append('rep' if current_speaker == 'BUSH' else 'dem')\n",
    "            data['left/right'].append('right' if current_speaker == 'BUSH' else 'left')\n",
    "            data['content'].append(re.sub(r'^BUSH:|KERRY:', '', paragraph.get_text()))\n",
    "\n",
    "    prez_df_2004_2 = pd.DataFrame(data)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(prez_df_2004_2)\n",
    "    print(prez_df_2004_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   name  year dem/rep left/right  \\\n",
      "0  BUSH  2000     rep      right   \n",
      "1  GORE  2000     dem       left   \n",
      "\n",
      "                                             content  \n",
      "0   I don’t think a president can do that. I was ...  \n",
      "1   Well, Jim, the FDA took 12 years, and I do su...  \n",
      "(2, 5)\n"
     ]
    }
   ],
   "source": [
    "#Scraped Data for Presidential Election 2000 (Gore and Bush)\n",
    "\n",
    "#October 8, 2004\n",
    "\n",
    "URL = \"https://www.debates.org/voter-education/debate-transcripts/october-3-2000-transcript/\" \n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "all_paragraphs = soup.find_all('p')\n",
    "\n",
    "# Find the indices of paragraphs containing the word \"abortion\"\n",
    "abortion_indices = [index for index, p in enumerate(all_paragraphs) if 'abortion' in p.get_text().lower()]\n",
    "\n",
    "if abortion_indices:\n",
    "    # Extract paragraphs between the first and last mention of \"abortion\"\n",
    "    start_index = abortion_indices[0]\n",
    "    end_index = abortion_indices[-1]\n",
    "    \n",
    "    paragraphs_between_abortion = all_paragraphs[start_index:end_index + 1]\n",
    "\n",
    "    # Create a DataFrame with columns for 'name', 'year', 'dem/rep', 'left/right', 'content'\n",
    "    data = {'name': [], 'year': [], 'dem/rep': [], 'left/right': [], 'content': []}\n",
    "\n",
    "    current_speaker = None\n",
    "\n",
    "    for paragraph in paragraphs_between_abortion:\n",
    "        # Skip paragraphs that start with \"MODERATOR::\"\n",
    "        if not paragraph.get_text().startswith(\"MODERATOR:\"):\n",
    "            # Determine 'name' and 'dem/rep' based on the speaker\n",
    "            if 'BUSH:' in paragraph.get_text():\n",
    "                current_speaker = 'BUSH'\n",
    "            elif 'GORE:' in paragraph.get_text():\n",
    "                current_speaker = 'GORE'\n",
    "            \n",
    "            data['name'].append(current_speaker)\n",
    "            data['year'].append(2000)\n",
    "            data['dem/rep'].append('rep' if current_speaker == 'BUSH' else 'dem')\n",
    "            data['left/right'].append('right' if current_speaker == 'BUSH' else 'left')\n",
    "            data['content'].append(re.sub(r'^BUSH:|GORE:', '', paragraph.get_text()))\n",
    "\n",
    "    prez_df_2000 = pd.DataFrame(data)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(prez_df_2000)\n",
    "    print(prez_df_2000.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Dataframes "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure there are 140 rows and 5 columns. This will be the merged dataframe for all the debates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       name  year dem/rep left/right  \\\n",
      "0     PENCE  2020     rep      right   \n",
      "1     PENCE  2020     rep      right   \n",
      "2     PENCE  2020     rep      right   \n",
      "3    HARRIS  2020     dem       left   \n",
      "4     PENCE  2020     rep      right   \n",
      "..      ...   ...     ...        ...   \n",
      "135    BUSH  2004     rep      right   \n",
      "136    BUSH  2004     rep      right   \n",
      "137    BUSH  2004     rep      right   \n",
      "138    BUSH  2000     rep      right   \n",
      "139    GORE  2000     dem       left   \n",
      "\n",
      "                                               content  \n",
      "0     Well thank you for the question, but I’ll use...  \n",
      "1     My hope is that when the hearing takes place,...  \n",
      "2     – treated respectfully and voted and confirme...  \n",
      "3     Thank you, Susan. First of all, Joe Biden and...  \n",
      "4     Well, thank you, Susan. Let me just say, addr...  \n",
      "..                                                 ...  \n",
      "135  Take, for example, the ban on partial birth ab...  \n",
      "136  What I’m saying is, is that as we promote life...  \n",
      "137  The last debate, my opponent said his wife was...  \n",
      "138   I don’t think a president can do that. I was ...  \n",
      "139   Well, Jim, the FDA took 12 years, and I do su...  \n",
      "\n",
      "[140 rows x 5 columns]\n",
      "(140, 5)\n"
     ]
    }
   ],
   "source": [
    "prez_merged = pd.concat([prez_df_2020, prez_df_2016, prez_df_2016_2, prez_df_2012, prez_df_2008, prez_df_2004_2, prez_df_2004, prez_df_2000], ignore_index=True)\n",
    "print(prez_merged)\n",
    "print(prez_merged.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze Top Words Used in Presidential Debate (Left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         word  count\n",
      "693     women     22\n",
      "537     right     16\n",
      "360      life     15\n",
      "233     faith     15\n",
      "543       roe     15\n",
      "323     issue     12\n",
      "158     court     12\n",
      "692     woman     12\n",
      "109  catholic     12\n",
      "116    choice     11\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Filter rows where 'dem/rep' is 'dem'\n",
    "dem_df = prez_merged[prez_merged['dem/rep'] == 'dem']\n",
    "\n",
    "# Combine all content into a single string\n",
    "left_content = ' '.join(dem_df['content'])\n",
    "\n",
    "#custom stopwords\n",
    "custom_stopwords = ['president','think','people','wade','said','make','would','know','abortion']\n",
    "\n",
    "# Create a CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words=stopwords.words('english')+custom_stopwords)\n",
    "\n",
    "# Fit and transform the content\n",
    "word_matrix = vectorizer.fit_transform([left_content])\n",
    "\n",
    "# Get feature names (words)\n",
    "left_words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get word counts\n",
    "word_counts = word_matrix.toarray()[0]\n",
    "\n",
    "# Create a DataFrame with words and counts\n",
    "word_df = pd.DataFrame({'word': left_words, 'count': word_counts})\n",
    "\n",
    "# Sort DataFrame by count in descending order\n",
    "word_df = word_df.sort_values(by='count', ascending=False)\n",
    "\n",
    "# Print the top words\n",
    "print(word_df.head(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Top Words Used in Presidential Debate (Right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word  count\n",
      "307     life     45\n",
      "31   america     16\n",
      "417      pro     15\n",
      "133    court     12\n",
      "581    voted     10\n",
      "77     birth     10\n",
      "422  promote     10\n",
      "520  supreme      9\n",
      "99     child      9\n",
      "508   states      9\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where 'dem/rep' is 'rep'\n",
    "rep_df = prez_merged[prez_merged['dem/rep'] == 'rep']\n",
    "\n",
    "# Combine all content into a single string\n",
    "right_content = ' '.join(rep_df['content'])\n",
    "\n",
    "#custom stopwords\n",
    "custom_stopwords = ['abortion','know','think','would','people','senator','abortions']\n",
    "\n",
    "# Create a CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words=stopwords.words('english')+custom_stopwords)\n",
    "\n",
    "# Fit and transform the content\n",
    "word_matrix = vectorizer.fit_transform([right_content])\n",
    "\n",
    "# Get feature names (words)\n",
    "right_words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get word counts\n",
    "word_counts = word_matrix.toarray()[0]\n",
    "\n",
    "# Create a DataFrame with words and counts\n",
    "word_df = pd.DataFrame({'word': right_words , 'count': word_counts})\n",
    "\n",
    "# Sort DataFrame by count in descending order\n",
    "word_df = word_df.sort_values(by='count', ascending=False)\n",
    "\n",
    "# Print the top words\n",
    "print(word_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Sentiment for Left:\n",
      "{'neg': 0.077, 'neu': 0.782, 'pos': 0.141, 'compound': 0.9997, 'Overall sentiment': 'positive'}\n",
      "\n",
      "Overall Sentiment for Right:\n",
      "{'neg': 0.066, 'neu': 0.743, 'pos': 0.192, 'compound': 0.9999, 'Overall sentiment': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def overall_sentiment(text,analyzer=SentimentIntensityAnalyzer()):  \n",
    "    \n",
    "    sentiment_dict = analyzer.polarity_scores(text)\n",
    "    comp = sentiment_dict['compound']\n",
    "    \n",
    "    #overall sentiment\n",
    "    if comp >= 0.05: \n",
    "        sentiment = 'positive'\n",
    "    elif comp <= -0.05: \n",
    "        sentiment = 'negative'\n",
    "    else: \n",
    "        sentiment = 'neutral'\n",
    "    sentiment_dict['Overall sentiment'] = sentiment \n",
    "    return sentiment_dict \n",
    "\n",
    "print(\"Overall Sentiment for Left:\")\n",
    "print(overall_sentiment(left_content))\n",
    "\n",
    "print(\"\\nOverall Sentiment for Right:\")\n",
    "print(overall_sentiment(right_content))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of **VADER**\n",
    "\n",
    "Both the left and right show highly positive VADER sentiment analysis scores. This makes sense consiering the context of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methods\n",
    "\n",
    "### DEADLINE: TUE 12/5 (Midnight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use Fightingwords to examine partisan difference in news outlets (FIGHTING WORDS LECTURE 10, use given library, used in PSET 3) - look at words \n",
    "- Use VADER on the presidential debates + congressional speech phrasings (VADER WAS FOUND IN PSET 1)\n",
    "- Use (k-means) Clustering of phrasings (apply to all) (FOUND IN PSET 2, LECTURE 6, 7)\n",
    "- Use BERT Classification (PSET 5, Lecture 15, 16, article discussed)\n",
    "- When dealing with CENTER, we would use VADER to examine the connotation of words used within the outlet and we also compare that to the fightingwords identified for left and right and examine the distribution of those words within that outlet\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "\n",
    "### DEADLINE: THURS 12/7 (Midnight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussion and conclusions\n",
    "\n",
    "### DEADLINE: 12/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info2950",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "be68b6d75416cba97d4f6ce2ab1e4721759e09cc82850453086f6f7bb9d313b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
